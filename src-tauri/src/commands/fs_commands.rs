//! NeoView - File System Commands
//! æ–‡ä»¶ç³»ç»Ÿæ“ä½œç›¸å…³çš„ Tauri å‘½ä»¤

use super::task_queue_commands::BackgroundSchedulerState;
use crate::core::cache_index_db::{CacheGcResult, CacheIndexDb, CacheIndexStats};
use crate::core::directory_cache::DirectoryCache;
use crate::core::fs_manager::FsItem;
use crate::core::{ArchiveManager, FsManager};
use log::{info, warn};
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use std::sync::Mutex;
use std::time::SystemTime;
use std::time::UNIX_EPOCH;
use tauri::async_runtime::spawn_blocking;
use tauri::State;

/// æ–‡ä»¶ç³»ç»ŸçŠ¶æ€
pub struct FsState {
    pub fs_manager: Arc<Mutex<FsManager>>,
    pub archive_manager: Arc<Mutex<ArchiveManager>>,
}

/// ç›®å½•ç¼“å­˜çŠ¶æ€ï¼ˆå†…å­˜ LRUï¼‰
pub struct DirectoryCacheState {
    pub cache: Mutex<DirectoryCache>,
}

/// ç¼“å­˜ç´¢å¼•çŠ¶æ€ï¼ˆSQLiteï¼‰
pub struct CacheIndexState {
    pub db: Arc<CacheIndexDb>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct FileInfo {
    pub name: String,
    pub path: String,
    pub is_directory: bool,
    pub size: Option<u64>,
    pub modified: Option<String>,
}

#[tauri::command]
pub async fn read_directory(
    path: String,
    excluded_paths: Option<Vec<String>>,
) -> Result<Vec<FileInfo>, String> {
    let path = Path::new(&path);
    let excluded = excluded_paths.unwrap_or_default();

    if !path.exists() {
        return Err(format!("Path does not exist: {}", path.display()));
    }

    if !path.is_dir() {
        return Err(format!("Path is not a directory: {}", path.display()));
    }

    let mut entries = Vec::new();

    let read_dir = fs::read_dir(path).map_err(|e| format!("Failed to read directory: {}", e))?;

    for entry in read_dir {
        // ä¼˜é›…å¤„ç†æƒé™é”™è¯¯
        let entry = match entry {
            Ok(e) => e,
            Err(e) => {
                log::debug!("è·³è¿‡æ— æ³•è¯»å–çš„æ¡ç›®: {}", e);
                continue;
            }
        };
        
        let entry_path = entry.path();
        let path_str = entry_path.to_string_lossy().to_string();
        
        // æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­ï¼ˆè§„èŒƒåŒ–è·¯å¾„è¿›è¡Œæ¯”è¾ƒï¼‰
        let normalized_path = path_str.replace('/', "\\");
        let is_excluded = excluded.iter().any(|ex| {
            let normalized_ex = ex.replace('/', "\\");
            normalized_path == normalized_ex 
                || normalized_path.starts_with(&format!("{}\\", normalized_ex))
        });
        
        if is_excluded {
            continue;
        }
        
        // ä¼˜é›…å¤„ç†å…ƒæ•°æ®è·å–å¤±è´¥
        let metadata = match entry.metadata() {
            Ok(m) => Some(m),
            Err(e) => {
                log::debug!("è·³è¿‡æ— æ³•è·å–å…ƒæ•°æ®çš„æ¡ç›® {:?}: {}", entry_path, e);
                continue;
            }
        };

        let file_info = FileInfo {
            name: entry_path
                .file_name()
                .and_then(|n| n.to_str())
                .unwrap_or("Unknown")
                .to_string(),
            path: path_str,
            is_directory: entry_path.is_dir(),
            size: metadata.as_ref().map(|m| m.len()),
            modified: metadata
                .and_then(|m| m.modified().ok())
                .and_then(|t| t.duration_since(std::time::UNIX_EPOCH).ok())
                .map(|d| d.as_secs().to_string()),
        };

        entries.push(file_info);
    }

    // æŒ‰åç§°æ’åºï¼Œç›®å½•åœ¨å‰
    entries.sort_by(|a, b| match (a.is_directory, b.is_directory) {
        (true, false) => std::cmp::Ordering::Less,
        (false, true) => std::cmp::Ordering::Greater,
        _ => a.name.cmp(&b.name),
    });

    Ok(entries)
}

#[tauri::command]
pub async fn get_file_info(path: String) -> Result<FileInfo, String> {
    let path = Path::new(&path);

    if !path.exists() {
        return Err(format!("Path does not exist: {}", path.display()));
    }

    let metadata = fs::metadata(path).map_err(|e| format!("Failed to get file metadata: {}", e))?;

    Ok(FileInfo {
        name: path
            .file_name()
            .and_then(|n| n.to_str())
            .unwrap_or("Unknown")
            .to_string(),
        path: path.to_string_lossy().to_string(),
        is_directory: path.is_dir(),
        size: Some(metadata.len()),
        modified: metadata
            .modified()
            .ok()
            .and_then(|t| t.duration_since(std::time::UNIX_EPOCH).ok())
            .map(|d| d.as_secs().to_string()),
    })
}

#[tauri::command]
pub async fn path_exists(path: String) -> Result<bool, String> {
    Ok(Path::new(&path).exists())
}

/// è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹
#[tauri::command]
pub async fn read_text_file(path: String) -> Result<String, String> {
    let path = Path::new(&path);
    
    if !path.exists() {
        return Err(format!("æ–‡ä»¶ä¸å­˜åœ¨: {}", path.display()));
    }
    
    fs::read_to_string(path).map_err(|e| format!("è¯»å–æ–‡ä»¶å¤±è´¥: {}", e))
}

/// æµè§ˆç›®å½•å†…å®¹ï¼ˆä½¿ç”¨æ–°çš„ FsManagerï¼‰
#[tauri::command]
pub async fn browse_directory(
    path: String,
    state: State<'_, FsState>,
) -> Result<Vec<crate::core::fs_manager::FsItem>, String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let path = PathBuf::from(path);
    fs_manager.read_directory(&path)
}

#[derive(Debug, Clone, Serialize)]
#[serde(rename_all = "camelCase")]
pub struct DirectorySnapshotResponse {
    pub items: Vec<FsItem>,
    pub mtime: Option<u64>,
    pub cached: bool,
}

fn directory_mtime(path: &Path) -> Option<u64> {
    let metadata = fs::metadata(path).ok()?;
    metadata
        .modified()
        .ok()
        .and_then(|t| t.duration_since(UNIX_EPOCH).ok())
        .map(|d| d.as_secs())
}

#[tauri::command]
pub async fn load_directory_snapshot(
    path: String,
    state: State<'_, FsState>,
    cache_state: State<'_, DirectoryCacheState>,
    cache_index: State<'_, CacheIndexState>,
    scheduler: State<'_, BackgroundSchedulerState>,
) -> Result<DirectorySnapshotResponse, String> {
    let path_buf = PathBuf::from(&path);
    let mtime = directory_mtime(&path_buf);

    // å†…å­˜ç¼“å­˜
    {
        // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
        let mut cache = cache_state
            .cache
            .lock()
            .unwrap_or_else(|e| e.into_inner());
        if let Some(entry) = cache.get(&path, mtime) {
            println!(
                "ğŸ“ DirectorySnapshot å‘½ä¸­å†…å­˜ç¼“å­˜: {} (entries={})",
                path,
                entry.items.len()
            );
            return Ok(DirectorySnapshotResponse {
                items: entry.items,
                mtime: entry.mtime,
                cached: true,
            });
        }
    }

    // SQLite ç¼“å­˜
    if let Some(persisted_items) = cache_index.db.load_directory_snapshot(&path, mtime)? {
        // println!(
        //     "ğŸ“ DirectorySnapshot å‘½ä¸­ SQLite ç¼“å­˜: {} (entries={})",
        //     path,
        //     persisted_items.len()
        // );
        {
            // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
            let mut cache = cache_state
                .cache
                .lock()
                .unwrap_or_else(|e| e.into_inner());
            cache.insert(path.clone(), persisted_items.clone(), mtime);
        }
        return Ok(DirectorySnapshotResponse {
            items: persisted_items,
            mtime,
            cached: true,
        });
    }

    // æ–‡ä»¶ç³»ç»Ÿè¯»å–
    println!(
        "ğŸ“ DirectorySnapshot miss: {} -> è°ƒåº¦ filebrowser-directory-load",
        path
    );
    let fs_manager = Arc::clone(&state.fs_manager);
    let job_path = path.clone();
    let path_for_job = path_buf.clone();
    let items: Vec<FsItem> = scheduler
        .scheduler
        .enqueue_blocking(
            "filebrowser-directory-load",
            job_path,
            move || -> Result<Vec<FsItem>, String> {
                // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
                let fs_manager = fs_manager
                    .lock()
                    .unwrap_or_else(|e| e.into_inner());
                fs_manager.read_directory(&path_for_job)
            },
        )
        .await?;

    {
        // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
        let mut cache = cache_state
            .cache
            .lock()
            .unwrap_or_else(|e| e.into_inner());
        cache.insert(path.clone(), items.clone(), mtime);
    }
    cache_index
        .db
        .save_directory_snapshot(&path, mtime, &items)?;

    Ok(DirectorySnapshotResponse {
        items,
        mtime,
        cached: false,
    })
}

/// æ‰¹é‡å¹¶å‘åŠ è½½å¤šä¸ªç›®å½•å¿«ç…§
/// ä½¿ç”¨ tokio::spawn å¹¶å‘æ‰§è¡Œï¼Œé¿å…ä¸²è¡Œé˜»å¡
#[derive(Debug, Clone, Serialize)]
#[serde(rename_all = "camelCase")]
pub struct BatchDirectorySnapshotResult {
    pub path: String,
    pub snapshot: Option<DirectorySnapshotResponse>,
    pub error: Option<String>,
}

#[tauri::command]
pub async fn batch_load_directory_snapshots(
    paths: Vec<String>,
    state: State<'_, FsState>,
    cache_state: State<'_, DirectoryCacheState>,
    cache_index: State<'_, CacheIndexState>,
    scheduler: State<'_, BackgroundSchedulerState>,
) -> Result<Vec<BatchDirectorySnapshotResult>, String> {
    use futures::future::join_all;

    let fs_manager = Arc::clone(&state.fs_manager);
    let cache_index_db = Arc::clone(&cache_index.db);
    // scheduler å‚æ•°ä¿ç•™ç”¨äºæœªæ¥æ‰©å±•
    let _ = &scheduler;

    // æ”¶é›†éœ€è¦ä»æ–‡ä»¶ç³»ç»ŸåŠ è½½çš„è·¯å¾„ï¼ˆç¼“å­˜ missï¼‰
    let mut results: Vec<BatchDirectorySnapshotResult> = Vec::with_capacity(paths.len());
    let mut pending_loads: Vec<(usize, String, PathBuf, Option<u64>)> = Vec::new();

    for (idx, path) in paths.iter().enumerate() {
        let path_buf = PathBuf::from(path);
        let mtime = directory_mtime(&path_buf);

        // 1. æ£€æŸ¥å†…å­˜ç¼“å­˜
        {
            let mut cache = cache_state
                .cache
                .lock()
                .unwrap_or_else(|e| e.into_inner());
            if let Some(entry) = cache.get(path, mtime) {
                results.push(BatchDirectorySnapshotResult {
                    path: path.clone(),
                    snapshot: Some(DirectorySnapshotResponse {
                        items: entry.items,
                        mtime: entry.mtime,
                        cached: true,
                    }),
                    error: None,
                });
                continue;
            }
        }

        // 2. æ£€æŸ¥ SQLite ç¼“å­˜
        match cache_index_db.load_directory_snapshot(path, mtime) {
            Ok(Some(persisted_items)) => {
                // å›å¡«å†…å­˜ç¼“å­˜
                {
                    let mut cache = cache_state
                        .cache
                        .lock()
                        .unwrap_or_else(|e| e.into_inner());
                    cache.insert(path.clone(), persisted_items.clone(), mtime);
                }
                results.push(BatchDirectorySnapshotResult {
                    path: path.clone(),
                    snapshot: Some(DirectorySnapshotResponse {
                        items: persisted_items,
                        mtime,
                        cached: true,
                    }),
                    error: None,
                });
                continue;
            }
            Ok(None) => {
                // éœ€è¦ä»æ–‡ä»¶ç³»ç»ŸåŠ è½½
                pending_loads.push((idx, path.clone(), path_buf, mtime));
                // å ä½
                results.push(BatchDirectorySnapshotResult {
                    path: path.clone(),
                    snapshot: None,
                    error: None,
                });
            }
            Err(e) => {
                results.push(BatchDirectorySnapshotResult {
                    path: path.clone(),
                    snapshot: None,
                    error: Some(e),
                });
            }
        }
    }

    if pending_loads.is_empty() {
        return Ok(results);
    }

    println!(
        "ğŸ“ BatchDirectorySnapshot: {} miss, {} å‘½ä¸­ç¼“å­˜ -> å¹¶å‘åŠ è½½",
        pending_loads.len(),
        paths.len() - pending_loads.len()
    );

    // 3. å¹¶å‘åŠ è½½æ‰€æœ‰ miss çš„ç›®å½•
    let futures: Vec<_> = pending_loads
        .into_iter()
        .map(|(idx, path, path_buf, mtime)| {
            let fs_manager = Arc::clone(&fs_manager);
            let cache_index_db = Arc::clone(&cache_index_db);
            let cache_state_inner = cache_state.inner().clone();

            async move {
                // ä½¿ç”¨ spawn_blocking é¿å…é˜»å¡ tokio çº¿ç¨‹
                let load_result = tauri::async_runtime::spawn_blocking(move || {
                    let fs = fs_manager.lock().unwrap_or_else(|e| e.into_inner());
                    fs.read_directory(&path_buf)
                })
                .await;

                let result = match load_result {
                    Ok(Ok(items)) => {
                        // å›å¡«ç¼“å­˜
                        {
                            let mut cache = cache_state_inner
                                .cache
                                .lock()
                                .unwrap_or_else(|e| e.into_inner());
                            cache.insert(path.clone(), items.clone(), mtime);
                        }
                        let _ = cache_index_db.save_directory_snapshot(&path, mtime, &items);

                        BatchDirectorySnapshotResult {
                            path,
                            snapshot: Some(DirectorySnapshotResponse {
                                items,
                                mtime,
                                cached: false,
                            }),
                            error: None,
                        }
                    }
                    Ok(Err(e)) => BatchDirectorySnapshotResult {
                        path,
                        snapshot: None,
                        error: Some(e),
                    },
                    Err(e) => BatchDirectorySnapshotResult {
                        path,
                        snapshot: None,
                        error: Some(format!("spawn_blocking error: {}", e)),
                    },
                };
                (idx, result)
            }
        })
        .collect();

    // å¹¶å‘æ‰§è¡Œæ‰€æœ‰åŠ è½½ä»»åŠ¡
    let loaded: Vec<(usize, BatchDirectorySnapshotResult)> = join_all(futures).await;

    // åˆå¹¶ç»“æœ
    for (idx, result) in loaded {
        results[idx] = result;
    }

    Ok(results)
}

/// è·å–ç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡
#[tauri::command]
pub async fn get_images_in_directory(
    path: String,
    recursive: bool,
    state: State<'_, FsState>,
) -> Result<Vec<String>, String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let path = PathBuf::from(path);
    let images = fs_manager.get_images_in_directory(&path, recursive)?;

    Ok(images
        .iter()
        .map(|p| p.to_string_lossy().to_string())
        .collect())
}

/// è·å–å•ä¸ªæ–‡ä»¶/æ–‡ä»¶å¤¹çš„å…ƒæ•°æ®ï¼ˆåŒ…å«åˆ›å»º/ä¿®æ”¹æ—¶é—´ï¼‰
#[tauri::command]
pub async fn get_file_metadata(
    path: String,
    state: State<'_, FsState>,
) -> Result<crate::core::fs_manager::FsItem, String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let path = PathBuf::from(path);
    fs_manager.get_file_metadata(&path)
}

/// åˆ›å»ºç›®å½•
#[tauri::command]
pub async fn create_directory(path: String, state: State<'_, FsState>) -> Result<(), String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let path = PathBuf::from(path);
    fs_manager.create_directory(&path)
}

/// åˆ é™¤æ–‡ä»¶æˆ–ç›®å½•
#[tauri::command]
pub async fn delete_path(path: String, state: State<'_, FsState>) -> Result<(), String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let path = PathBuf::from(path);
    fs_manager.delete(&path)
}

/// é‡å‘½åæ–‡ä»¶æˆ–ç›®å½•
#[tauri::command]
pub async fn rename_path(
    from: String,
    to: String,
    state: State<'_, FsState>,
) -> Result<(), String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let from_path = PathBuf::from(from);
    let to_path = PathBuf::from(to);
    fs_manager.rename(&from_path, &to_path)
}

/// ç§»åŠ¨åˆ°å›æ”¶ç«™
#[tauri::command]
pub async fn move_to_trash(path: String, state: State<'_, FsState>) -> Result<(), String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let path = PathBuf::from(path);
    fs_manager.move_to_trash(&path)
}

// ===== å‹ç¼©åŒ…ç›¸å…³å‘½ä»¤ =====

/// åˆ—å‡ºå‹ç¼©åŒ…å†…å®¹
#[tauri::command]
pub async fn list_archive_contents(
    archive_path: String,
    state: State<'_, FsState>,
) -> Result<Vec<crate::core::archive::ArchiveEntry>, String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let archive_manager = state
        .archive_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let path = PathBuf::from(archive_path);
    // ä½¿ç”¨ list_contents è‡ªåŠ¨æ£€æµ‹æ ¼å¼ï¼ˆæ”¯æŒ ZIP/RAR/7zï¼‰
    archive_manager.list_contents(&path)
}

/// åˆ é™¤å‹ç¼©åŒ…ä¸­çš„æŒ‡å®šæ¡ç›®
#[tauri::command]
pub async fn delete_archive_entry(
    archive_path: String,
    inner_path: String,
    state: State<'_, FsState>,
) -> Result<(), String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let archive_manager = state
        .archive_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let path = PathBuf::from(&archive_path);
    archive_manager.delete_entry_from_zip(&path, &inner_path)
}

/// ã€ä¼˜åŒ–ã€‘ä»å‹ç¼©åŒ…åŠ è½½å›¾ç‰‡ - ä½¿ç”¨ Response ç›´æ¥ä¼ è¾“äºŒè¿›åˆ¶
/// é¿å… Vec<u8> -> JSON Array çš„åºåˆ—åŒ–å¼€é”€
#[tauri::command]
pub async fn load_image_from_archive_binary(
    archive_path: String,
    file_path: String,
    trace_id: Option<String>,
    page_index: Option<i32>,
    state: State<'_, FsState>,
) -> Result<tauri::ipc::Response, String> {
    let trace_id = trace_id.unwrap_or_else(|| {
        let millis = SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map(|d| d.as_millis())
            .unwrap_or_default();
        format!("rust-archive-bin-{}-{}", page_index.unwrap_or(-1), millis)
    });

    info!(
        "ğŸ“¥ [ImagePipeline:{}] load_image_from_archive_binary request archive={} inner={} page_index={:?}",
        trace_id, archive_path, file_path, page_index
    );

    let archive_manager = Arc::clone(&state.archive_manager);
    let archive_path_buf = PathBuf::from(&archive_path);
    let inner_path = file_path.clone();
    let result = spawn_blocking(move || {
        // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
        let manager = archive_manager
            .lock()
            .unwrap_or_else(|e| e.into_inner());
        manager.load_image_from_archive_binary(&archive_path_buf, &inner_path)
    })
    .await
    .map_err(|e| format!("load_image_from_archive_binary join error: {}", e))?;

    match &result {
        Ok(bytes) => {
            info!(
                "ğŸ“¤ [ImagePipeline:{}] load_image_from_archive_binary success bytes={}",
                trace_id,
                bytes.len()
            );
            // ä½¿ç”¨ Response ç›´æ¥ä¼ è¾“äºŒè¿›åˆ¶æ•°æ®ï¼Œé¿å… JSON åºåˆ—åŒ–
            Ok(tauri::ipc::Response::new(bytes.clone()))
        },
        Err(err) => {
            warn!(
                "âš ï¸ [ImagePipeline:{}] load_image_from_archive_binary failed: {}",
                trace_id, err
            );
            Err(err.clone())
        }
    }
}

/// ä»å‹ç¼©åŒ…åŠ è½½å›¾ç‰‡ (å…¼å®¹æ—§ç‰ˆ)
#[tauri::command]
pub async fn load_image_from_archive(
    archive_path: String,
    file_path: String,
    trace_id: Option<String>,
    page_index: Option<i32>,
    state: State<'_, FsState>,
) -> Result<Vec<u8>, String> {
    let trace_id = trace_id.unwrap_or_else(|| {
        let millis = SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map(|d| d.as_millis())
            .unwrap_or_default();
        format!("rust-archive-{}-{}", page_index.unwrap_or(-1), millis)
    });

    info!(
        "ğŸ“¥ [ImagePipeline:{}] load_image_from_archive request archive={} inner={} page_index={:?}",
        trace_id, archive_path, file_path, page_index
    );

    let archive_manager = Arc::clone(&state.archive_manager);
    let archive_path_buf = PathBuf::from(&archive_path);
    let inner_path = file_path.clone();
    let result = spawn_blocking(move || {
        // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
        let manager = archive_manager
            .lock()
            .unwrap_or_else(|e| e.into_inner());
        manager.load_image_from_archive_binary(&archive_path_buf, &inner_path)
    })
    .await
    .map_err(|e| format!("load_image_from_archive join error: {}", e))?;

    match &result {
        Ok(bytes) => info!(
            "ğŸ“¤ [ImagePipeline:{}] load_image_from_archive success bytes={}",
            trace_id,
            bytes.len()
        ),
        Err(err) => warn!(
            "âš ï¸ [ImagePipeline:{}] load_image_from_archive failed: {}",
            trace_id, err
        ),
    }

    result
}

/// ã€ä¼˜åŒ–ã€‘ä»å‹ç¼©åŒ…è§£å‹å›¾ç‰‡åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œè¿”å›ä¸´æ—¶æ–‡ä»¶è·¯å¾„
/// å‰ç«¯å¯ä»¥ä½¿ç”¨ convertFileSrc ç›´æ¥è®¿é—®ï¼Œç»•è¿‡ IPC åºåˆ—åŒ–
#[tauri::command]
pub async fn extract_image_to_temp(
    archive_path: String,
    file_path: String,
    trace_id: Option<String>,
    page_index: Option<i32>,
    state: State<'_, FsState>,
) -> Result<String, String> {
    let trace_id = trace_id.unwrap_or_else(|| {
        let millis = SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map(|d| d.as_millis())
            .unwrap_or_default();
        format!("rust-extract-{}-{}", page_index.unwrap_or(-1), millis)
    });

    info!(
        "ğŸ“¥ [ImagePipeline:{}] extract_image_to_temp request archive={} inner={} page_index={:?}",
        trace_id, archive_path, file_path, page_index
    );

    let archive_manager = Arc::clone(&state.archive_manager);
    let archive_path_buf = PathBuf::from(&archive_path);
    let inner_path = file_path.clone();
    
    let result = spawn_blocking(move || {
        // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
        let manager = archive_manager
            .lock()
            .unwrap_or_else(|e| e.into_inner());
        
        // è¯»å–å›¾ç‰‡æ•°æ®ï¼ˆæ”¯æŒ ZIP/RAR/7zï¼‰
        let bytes = manager.load_image_from_archive_binary(&archive_path_buf, &inner_path)?;
        
        // è·å–æ–‡ä»¶æ‰©å±•å
        let ext = Path::new(&inner_path)
            .extension()
            .and_then(|e| e.to_str())
            .unwrap_or("jpg");
        
        // åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        let temp_dir = std::env::temp_dir().join("neoview_cache");
        std::fs::create_dir_all(&temp_dir).map_err(|e| format!("åˆ›å»ºä¸´æ—¶ç›®å½•å¤±è´¥: {}", e))?;
        
        // ä½¿ç”¨ hash ä½œä¸ºæ–‡ä»¶åï¼Œé¿å…é‡å¤è§£å‹
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        let mut hasher = DefaultHasher::new();
        archive_path_buf.hash(&mut hasher);
        inner_path.hash(&mut hasher);
        let hash = hasher.finish();
        
        let temp_path = temp_dir.join(format!("{:x}.{}", hash, ext));
        
        // å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›è·¯å¾„
        if temp_path.exists() {
            return Ok(temp_path.to_string_lossy().to_string());
        }
        
        // å†™å…¥ä¸´æ—¶æ–‡ä»¶
        std::fs::write(&temp_path, &bytes).map_err(|e| format!("å†™å…¥ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {}", e))?;
        
        Ok(temp_path.to_string_lossy().to_string())
    })
    .await
    .map_err(|e| format!("extract_image_to_temp join error: {}", e))?;

    match &result {
        Ok(path) => info!(
            "ğŸ“¤ [ImagePipeline:{}] extract_image_to_temp success path={}",
            trace_id, path
        ),
        Err(err) => warn!(
            "âš ï¸ [ImagePipeline:{}] extract_image_to_temp failed: {}",
            trace_id, err
        ),
    }

    result
}

/// è·å–å‹ç¼©åŒ…ä¸­çš„æ‰€æœ‰å›¾ç‰‡
#[tauri::command]
pub async fn get_images_from_archive(
    archive_path: String,
    state: State<'_, FsState>,
) -> Result<Vec<String>, String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let archive_manager = state
        .archive_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let path = PathBuf::from(archive_path);
    // ä½¿ç”¨ get_images_from_archive æ”¯æŒ ZIP/RAR/7z
    archive_manager.get_images_from_archive(&path)
}

/// ã€ä¼˜åŒ–ã€‘æ‰¹é‡é¢„è§£å‹å‹ç¼©åŒ…ä¸­çš„å›¾ç‰‡åˆ°ä¸´æ—¶ç›®å½•
/// è¿”å›ä¸´æ—¶ç›®å½•è·¯å¾„ï¼Œå‰ç«¯å¯ä»¥ç›´æ¥ç”¨ convertFileSrc è®¿é—®
#[tauri::command]
pub async fn batch_extract_archive(
    archive_path: String,
    state: State<'_, FsState>,
) -> Result<String, String> {
    use std::collections::hash_map::DefaultHasher;
    use std::hash::{Hash, Hasher};

    let archive_path_buf = PathBuf::from(&archive_path);
    
    // è®¡ç®—å‹ç¼©åŒ…çš„ hash ä½œä¸ºä¸´æ—¶ç›®å½•å
    let mut hasher = DefaultHasher::new();
    archive_path_buf.hash(&mut hasher);
    let hash = hasher.finish();
    
    let temp_dir = std::env::temp_dir()
        .join("neoview_cache")
        .join(format!("{:x}", hash));
    
    // å¦‚æœç›®å½•å·²å­˜åœ¨ä¸”æœ‰å†…å®¹ï¼Œç›´æ¥è¿”å›
    if temp_dir.exists() {
        let count = std::fs::read_dir(&temp_dir)
            .map(|d| d.count())
            .unwrap_or(0);
        if count > 0 {
            info!("ğŸ“¦ ä½¿ç”¨å·²è§£å‹çš„ç¼“å­˜ç›®å½•: {:?} ({} files)", temp_dir, count);
            return Ok(temp_dir.to_string_lossy().to_string());
        }
    }
    
    info!("ğŸ“¦ å¼€å§‹æ‰¹é‡è§£å‹: {:?} -> {:?}", archive_path_buf, temp_dir);
    
    let archive_manager = Arc::clone(&state.archive_manager);
    
    let result = spawn_blocking(move || {
        // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
        let manager = archive_manager
            .lock()
            .unwrap_or_else(|e| e.into_inner());
        
        // è·å–æ‰€æœ‰å›¾ç‰‡ï¼ˆæ”¯æŒ ZIP/RAR/7zï¼‰
        let images = manager.get_images_from_archive(&archive_path_buf)?;
        
        // åˆ›å»ºä¸´æ—¶ç›®å½•
        std::fs::create_dir_all(&temp_dir).map_err(|e| format!("åˆ›å»ºä¸´æ—¶ç›®å½•å¤±è´¥: {}", e))?;
        
        // è§£å‹æ‰€æœ‰å›¾ç‰‡
        for (index, inner_path) in images.iter().enumerate() {
            let bytes = manager.load_image_from_archive_binary(&archive_path_buf, inner_path)?;
            
            // ä½¿ç”¨ç´¢å¼•ä½œä¸ºæ–‡ä»¶åï¼Œä¿æŒé¡ºåº
            let ext = Path::new(inner_path)
                .extension()
                .and_then(|e| e.to_str())
                .unwrap_or("jpg");
            let temp_file = temp_dir.join(format!("{:05}.{}", index, ext));
            
            std::fs::write(&temp_file, &bytes).map_err(|e| format!("å†™å…¥ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {}", e))?;
        }
        
        info!("âœ… æ‰¹é‡è§£å‹å®Œæˆ: {} files", images.len());
        Ok(temp_dir.to_string_lossy().to_string())
    })
    .await
    .map_err(|e| format!("batch_extract_archive join error: {}", e))?;
    
    result
}

/// æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„å‹ç¼©åŒ…
#[tauri::command]
pub async fn is_supported_archive(path: String) -> Result<bool, String> {
    let path = PathBuf::from(path);
    Ok(crate::core::archive::ArchiveManager::is_supported_archive(
        &path,
    ))
}

/// æ‰¹é‡æ‰«æå‹ç¼©åŒ…å†…å®¹
/// é€šè¿‡ Rust è°ƒåº¦å™¨æ‰§è¡Œï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
#[tauri::command]
pub async fn batch_scan_archives(
    archive_paths: Vec<String>,
    state: State<'_, FsState>,
    scheduler: State<'_, BackgroundSchedulerState>,
) -> Result<Vec<ArchiveScanResult>, String> {
    let archive_manager = Arc::clone(&state.archive_manager);
    let paths: Vec<PathBuf> = archive_paths.iter().map(PathBuf::from).collect();

    let results: Vec<ArchiveScanResult> = scheduler
        .scheduler
        .enqueue_blocking(
            "archive-batch-scan",
            "filebrowser",
            move || -> Result<Vec<ArchiveScanResult>, String> {
                let mut results = Vec::with_capacity(paths.len());
                // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
                let manager = archive_manager
                    .lock()
                    .unwrap_or_else(|e| e.into_inner());

                for path in paths {
                    let archive_path_str = path.to_string_lossy().to_string();
                    // ä½¿ç”¨ list_contents è‡ªåŠ¨æ£€æµ‹æ ¼å¼ï¼ˆæ”¯æŒ ZIP/RAR/7zï¼‰
                    match manager.list_contents(&path) {
                        Ok(entries) => {
                            results.push(ArchiveScanResult {
                                archive_path: archive_path_str,
                                entries,
                                error: None,
                            });
                        }
                        Err(e) => {
                            results.push(ArchiveScanResult {
                                archive_path: archive_path_str,
                                entries: Vec::new(),
                                error: Some(e),
                            });
                        }
                    }
                }

                Ok(results)
            },
        )
        .await?;

    Ok(results)
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct ArchiveScanResult {
    pub archive_path: String,
    pub entries: Vec<crate::core::archive::ArchiveEntry>,
    pub error: Option<String>,
}

/// ã€ä¼˜åŒ–ã€‘å¹¶è¡Œé¢„åŠ è½½å¤šä¸ªé¡µé¢åˆ°ç¼“å­˜
/// ä½¿ç”¨ rayon å¹¶è¡Œè§£å‹ï¼Œæå‡é¢„åŠ è½½é€Ÿåº¦
#[tauri::command]
pub async fn preload_archive_pages(
    archive_path: String,
    page_paths: Vec<String>,
    state: State<'_, FsState>,
) -> Result<PreloadResult, String> {
    use rayon::prelude::*;
    use std::sync::atomic::{AtomicUsize, Ordering};
    
    let archive_manager = Arc::clone(&state.archive_manager);
    let archive_path_buf = PathBuf::from(&archive_path);
    let page_count = page_paths.len();
    
    info!(
        "ğŸ“¦ [Preload] å¼€å§‹å¹¶è¡Œé¢„åŠ è½½ {} ä¸ªé¡µé¢: {}",
        page_count,
        archive_path
    );
    
    let start_time = std::time::Instant::now();
    
    let result = spawn_blocking(move || {
        // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
        let manager = archive_manager
            .lock()
            .unwrap_or_else(|e| e.into_inner());
        
        let success_count = AtomicUsize::new(0);
        let total_bytes = AtomicUsize::new(0);
        
        // ä½¿ç”¨ rayon å¹¶è¡Œè§£å‹
        let errors: Vec<String> = page_paths
            .par_iter()
            .filter_map(|page_path| {
                match manager.load_image_from_archive_binary(&archive_path_buf, page_path) {
                    Ok(bytes) => {
                        success_count.fetch_add(1, Ordering::Relaxed);
                        total_bytes.fetch_add(bytes.len(), Ordering::Relaxed);
                        None
                    }
                    Err(e) => Some(format!("{}: {}", page_path, e)),
                }
            })
            .collect();
        
        Ok(PreloadResult {
            total: page_count,
            success: success_count.load(Ordering::Relaxed),
            failed: errors.len(),
            total_bytes: total_bytes.load(Ordering::Relaxed),
            errors: if errors.is_empty() { None } else { Some(errors) },
        })
    })
    .await
    .map_err(|e| format!("preload_archive_pages join error: {}", e))?;
    
    let elapsed = start_time.elapsed();
    
    match &result {
        Ok(r) => info!(
            "âœ… [Preload] å®Œæˆ: {}/{} æˆåŠŸ, {} bytes, {:.1}ms",
            r.success, r.total, r.total_bytes, elapsed.as_secs_f64() * 1000.0
        ),
        Err(e) => warn!("âš ï¸ [Preload] å¤±è´¥: {}", e),
    }
    
    result
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct PreloadResult {
    pub total: usize,
    pub success: usize,
    pub failed: usize,
    pub total_bytes: usize,
    pub errors: Option<Vec<String>>,
}

// ===== æ–‡ä»¶æ“ä½œå‘½ä»¤ =====

/// å¤åˆ¶æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹
#[tauri::command]
pub async fn copy_path(from: String, to: String, state: State<'_, FsState>) -> Result<(), String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let from_path = PathBuf::from(from);
    let to_path = PathBuf::from(to);
    fs_manager.copy(&from_path, &to_path)
}

/// ç§»åŠ¨æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹
#[tauri::command]
pub async fn move_path(from: String, to: String, state: State<'_, FsState>) -> Result<(), String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let from_path = PathBuf::from(from);
    let to_path = PathBuf::from(to);
    fs_manager.move_item(&from_path, &to_path)
}

/// åœ¨ç³»ç»Ÿé»˜è®¤ç¨‹åºä¸­æ‰“å¼€æ–‡ä»¶
#[tauri::command]
pub async fn open_with_system(path: String) -> Result<(), String> {
    #[cfg(target_os = "windows")]
    {
        std::process::Command::new("cmd")
            .args(["/C", "start", "", &path])
            .spawn()
            .map_err(|e| format!("Failed to open file: {}", e))?;
    }

    #[cfg(target_os = "macos")]
    {
        std::process::Command::new("open")
            .arg(&path)
            .spawn()
            .map_err(|e| format!("Failed to open file: {}", e))?;
    }

    #[cfg(target_os = "linux")]
    {
        std::process::Command::new("xdg-open")
            .arg(&path)
            .spawn()
            .map_err(|e| format!("Failed to open file: {}", e))?;
    }

    Ok(())
}

/// åœ¨æ–‡ä»¶ç®¡ç†å™¨ä¸­æ˜¾ç¤ºæ–‡ä»¶
#[tauri::command]
pub async fn show_in_file_manager(path: String) -> Result<(), String> {
    log::info!("show_in_file_manager called with path: {}", path);
    
    let path = PathBuf::from(&path);
    
    // æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if !path.exists() {
        log::warn!("Path does not exist: {}", path.display());
        return Err(format!("Path does not exist: {}", path.display()));
    }

    #[cfg(target_os = "windows")]
    {
        // canonicalize ä¼šæ·»åŠ  \\?\ å‰ç¼€ï¼Œexplorer ä¸æ”¯æŒï¼Œéœ€è¦ç§»é™¤
        let canonical_path = path.canonicalize()
            .map_err(|e| format!("Failed to canonicalize path: {}", e))?;
        let path_str = canonical_path.to_string_lossy();
        
        // ç§»é™¤ Windows æ‰©å±•è·¯å¾„å‰ç¼€ \\?\
        let clean_path = if path_str.starts_with(r"\\?\") {
            &path_str[4..]
        } else {
            &path_str
        };
        
        log::info!("Clean path for explorer: {}", clean_path);
        
        // GitButler æ–¹å¼: /select, å’Œè·¯å¾„ä½œä¸ºä¸¤ä¸ªç‹¬ç«‹å‚æ•°
        std::process::Command::new("explorer")
            .arg("/select,")
            .arg(clean_path)
            .status()
            .map_err(|e| format!("Failed to show in file manager: {}", e))?;
    }

    #[cfg(target_os = "macos")]
    {
        std::process::Command::new("open")
            .arg("-R")
            .arg(path.to_string_lossy().as_ref())
            .spawn()
            .map_err(|e| format!("Failed to show in file manager: {}", e))?;
    }

    #[cfg(target_os = "linux")]
    {
        // å¯¹äº Linuxï¼Œå°è¯•æ‰“å¼€åŒ…å«æ–‡ä»¶çš„ç›®å½•
        let parent = path
            .parent()
            .ok_or_else(|| "Cannot get parent directory".to_string())?;

        std::process::Command::new("xdg-open")
            .arg(parent.to_string_lossy().as_ref())
            .spawn()
            .map_err(|e| format!("Failed to show in file manager: {}", e))?;
    }

    Ok(())
}

/// æœç´¢æ–‡ä»¶ï¼ˆä½¿ç”¨åç«¯å®ç°ï¼Œä¸å†ä¾èµ– fd CLIï¼‰
#[tauri::command]
pub async fn search_files(
    path: String,
    query: String,
    options: Option<SearchOptions>,
    state: State<'_, FsState>,
) -> Result<Vec<crate::core::fs_manager::FsItem>, String> {
    let search_options = options.unwrap_or_default();

    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let path_buf = PathBuf::from(path);

    // è½¬æ¢ SearchOptions ç±»å‹
    let fs_search_options = crate::core::fs_manager::SearchOptions {
        include_subfolders: search_options.include_subfolders,
        max_results: search_options.max_results,
        search_in_path: search_options.search_in_path,
    };

    // ä½¿ç”¨ fs_manager çš„ search_files æ–¹æ³•ï¼ˆæ”¯æŒç´¢å¼•å’Œé€’å½’æœç´¢ï¼‰
    fs_manager.search_files(&path_buf, &query, &fs_search_options)
}

/// æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºå›¾ç‰‡
fn is_image_file(path: &Path) -> bool {
    if let Some(ext) = path.extension() {
        let ext = ext.to_string_lossy().to_lowercase();
        matches!(
            ext.as_str(),
            "jpg" | "jpeg" | "png" | "gif" | "bmp" | "webp" | "avif" | "jxl" | "tiff" | "tif"
        )
    } else {
        false
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, Default)]
#[serde(rename_all = "camelCase")]
pub struct SearchOptions {
    pub include_subfolders: Option<bool>,
    pub max_results: Option<usize>,
    pub search_in_path: Option<bool>, // æ˜¯å¦åœ¨å®Œæ•´è·¯å¾„ä¸­æœç´¢ï¼ˆè€Œä¸ä»…ä»…æ˜¯æ–‡ä»¶åï¼‰
}

/// åˆå§‹åŒ–æ–‡ä»¶ç´¢å¼•
#[tauri::command]
pub async fn initialize_file_index(state: State<'_, FsState>) -> Result<(), String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    fs_manager.initialize_indexer()
}

/// æ„å»ºæ–‡ä»¶ç´¢å¼•
#[tauri::command]
pub async fn build_file_index(
    path: String,
    recursive: bool,
    state: State<'_, FsState>,
) -> Result<(), String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let path = PathBuf::from(path);
    fs_manager.build_index(&path, recursive)
}

/// è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
#[tauri::command]
pub async fn get_index_stats(
    state: State<'_, FsState>,
) -> Result<crate::core::file_indexer::IndexStats, String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    fs_manager.get_index_stats()
}

/// æ¸…é™¤æ–‡ä»¶ç´¢å¼•
#[tauri::command]
pub async fn clear_file_index(state: State<'_, FsState>) -> Result<(), String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    fs_manager.clear_index()
}

/// åœ¨ç´¢å¼•ä¸­æœç´¢æ–‡ä»¶
#[tauri::command]
pub async fn search_in_index(
    query: String,
    max_results: Option<usize>,
    options: Option<IndexSearchOptions>,
    state: State<'_, FsState>,
) -> Result<Vec<crate::core::fs_manager::FsItem>, String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let max_results = max_results.unwrap_or(100);
    let search_options = options.map(|o| crate::core::file_indexer::SearchOptions {
        include_subfolders: o.include_subfolders.unwrap_or(true),
        images_only: o.images_only.unwrap_or(false),
        folders_only: o.folders_only.unwrap_or(false),
        min_size: o.min_size,
        max_size: o.max_size,
        modified_after: o.modified_after,
        modified_before: o.modified_before,
    });

    fs_manager.search_in_index(&query, max_results, search_options.as_ref())
}

/// è·å–ç´¢å¼•ä¸­çš„è·¯å¾„åˆ—è¡¨
#[tauri::command]
pub async fn get_indexed_paths(
    path: Option<String>,
    recursive: Option<bool>,
    state: State<'_, FsState>,
) -> Result<Vec<String>, String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let recursive = recursive.unwrap_or(false);

    fs_manager.get_indexed_paths(path.as_deref(), recursive)
}

/// æ£€æŸ¥è·¯å¾„æ˜¯å¦å·²è¢«ç´¢å¼•
#[tauri::command]
pub async fn is_path_indexed(path: String, state: State<'_, FsState>) -> Result<bool, String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    fs_manager.is_path_indexed(&path)
}

/// è·å–ç´¢å¼•è¿›åº¦
#[tauri::command]
pub async fn get_index_progress(
    state: State<'_, FsState>,
) -> Result<crate::core::file_indexer::IndexProgress, String> {
    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    fs_manager.get_index_progress()
}

/// ç´¢å¼•æœç´¢é€‰é¡¹
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct IndexSearchOptions {
    pub include_subfolders: Option<bool>,
    pub images_only: Option<bool>,
    pub folders_only: Option<bool>,
    pub min_size: Option<u64>,
    pub max_size: Option<u64>,
    pub modified_after: Option<u64>,
    pub modified_before: Option<u64>,
}

/// è·å–æœªç´¢å¼•çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹
#[tauri::command]
pub async fn get_unindexed_files(
    root_path: String,
    state: State<'_, FsState>,
) -> Result<UnindexedFilesResult, String> {
    println!("ğŸ” å¼€å§‹æ‰«ææœªç´¢å¼•æ–‡ä»¶: {}", root_path);

    // ä½¿ç”¨ unwrap_or_else æ¢å¤è¢«æ±¡æŸ“çš„é”
    let fs_manager = state
        .fs_manager
        .lock()
        .unwrap_or_else(|e| e.into_inner());

    let root_path = PathBuf::from(root_path);

    // æ£€æŸ¥æ ¹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if !root_path.exists() {
        return Err(format!("æ ¹è·¯å¾„ä¸å­˜åœ¨: {}", root_path.display()));
    }

    println!("ğŸ“ æ ¹è·¯å¾„å­˜åœ¨ï¼Œå¼€å§‹æ‰«æ...");

    // è·å–æ‰€æœ‰æ–‡ä»¶å’Œæ–‡ä»¶å¤¹
    let mut files = Vec::new();
    let mut folders = Vec::new();
    let mut archives = Vec::new();

    // é€’å½’æ‰«æç›®å½•
    scan_directory(
        &root_path,
        &mut files,
        &mut folders,
        &mut archives,
        &fs_manager,
    )?;

    println!(
        "ğŸ“Š æ‰«æå®Œæˆ: æ‰¾åˆ° {} ä¸ªæ–‡ä»¶, {} ä¸ªæ–‡ä»¶å¤¹",
        files.len(),
        folders.len()
    );

    // è¿‡æ»¤æ‰å·²ç´¢å¼•çš„é¡¹ç›®ï¼ˆåªè·å–æœªç´¢å¼•çš„ï¼‰
    let mut unindexed_files = Vec::new();
    let mut unindexed_folders = Vec::new();
    let mut unindexed_archives = Vec::new();

    for file in files {
        let path_str = file.to_string_lossy();
        match fs_manager.is_path_indexed(&path_str) {
            Ok(is_indexed) => {
                if !is_indexed {
                    unindexed_files.push(path_str.to_string());
                }
            }
            Err(e) => {
                println!("âš ï¸ æ£€æŸ¥ç´¢å¼•çŠ¶æ€å¤±è´¥ {}: {}", path_str, e);
                // å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œå‡è®¾æœªç´¢å¼•
                unindexed_files.push(path_str.to_string());
            }
        }
    }

    for folder in folders {
        let path_str = folder.to_string_lossy();
        match fs_manager.is_path_indexed(&path_str) {
            Ok(is_indexed) => {
                if !is_indexed {
                    unindexed_folders.push(path_str.to_string());
                }
            }
            Err(e) => {
                println!("âš ï¸ æ£€æŸ¥ç´¢å¼•çŠ¶æ€å¤±è´¥ {}: {}", path_str, e);
                // å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œå‡è®¾æœªç´¢å¼•
                unindexed_folders.push(path_str.to_string());
            }
        }
    }

    for archive in archives {
        let path_str = archive.to_string_lossy();
        match fs_manager.is_path_indexed(&path_str) {
            Ok(is_indexed) => {
                if !is_indexed {
                    unindexed_archives.push(path_str.to_string());
                }
            }
            Err(e) => {
                println!("âš ï¸ æ£€æŸ¥ç´¢å¼•çŠ¶æ€å¤±è´¥ {}: {}", path_str, e);
                unindexed_archives.push(path_str.to_string());
            }
        }
    }

    println!(
        "âœ… è¿‡æ»¤å®Œæˆ: æœªç´¢å¼•æ–‡ä»¶ {} ä¸ª, æœªç´¢å¼•æ–‡ä»¶å¤¹ {} ä¸ª, æœªç´¢å¼•å‹ç¼©åŒ… {} ä¸ª",
        unindexed_files.len(),
        unindexed_folders.len(),
        unindexed_archives.len()
    );

    Ok(UnindexedFilesResult {
        files: unindexed_files,
        folders: unindexed_folders,
        archives: unindexed_archives,
    })
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct UnindexedFilesResult {
    pub files: Vec<String>,
    pub folders: Vec<String>,
    pub archives: Vec<String>,
}

fn scan_directory(
    dir: &Path,
    files: &mut Vec<PathBuf>,
    folders: &mut Vec<PathBuf>,
    archives: &mut Vec<PathBuf>,
    fs_manager: &FsManager,
) -> Result<(), String> {
    let dir_name = dir.file_name().and_then(|n| n.to_str()).unwrap_or("æœªçŸ¥");

    println!("ğŸ“‚ æ‰«æç›®å½•: {}", dir.display());

    let entries =
        std::fs::read_dir(dir).map_err(|e| format!("è¯»å–ç›®å½•å¤±è´¥ {}: {}", dir.display(), e))?;

    let mut file_count = 0;
    let mut folder_count = 0;
    let mut archive_count = 0;

    for entry in entries {
        let entry = entry.map_err(|e| format!("è¯»å–æ¡ç›®å¤±è´¥: {}", e))?;
        let path = entry.path();

        // è·³è¿‡éšè—æ–‡ä»¶å’Œç³»ç»Ÿç›®å½•
        if let Some(name) = path.file_name() {
            let name_str = name.to_string_lossy();
            if name_str.starts_with('.')
                || name_str == "$RECYCLE.BIN"
                || name_str == "System Volume Information"
            {
                continue;
            }
        }

        if path.is_dir() {
            // æ·»åŠ æ–‡ä»¶å¤¹
            folders.push(path.clone());
            folder_count += 1;

            // é€’å½’æ‰«æå­ç›®å½•
            scan_directory(&path, files, folders, archives, fs_manager)?;
        } else if path.is_file() {
            // æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡æ–‡ä»¶æˆ–å‹ç¼©åŒ…
            if is_image_file(&path) {
                files.push(path);
                file_count += 1;
            } else if is_archive_file(&path) {
                archives.push(path);
                archive_count += 1;
            }
        }
    }

    if file_count > 0 || folder_count > 0 || archive_count > 0 {
        println!(
            "  ğŸ“Š {} - æ–‡ä»¶: {}, æ–‡ä»¶å¤¹: {}, å‹ç¼©åŒ…: {}",
            dir_name, file_count, folder_count, archive_count
        );
    }

    Ok(())
}

fn is_archive_file(path: &Path) -> bool {
    if let Some(ext) = path.extension() {
        let ext = ext.to_string_lossy().to_lowercase();
        matches!(ext.as_str(), "zip" | "cbz" | "rar" | "cbr" | "7z" | "cb7")
    } else {
        false
    }
}

// ===== åˆ†é¡µå’Œæµå¼æµè§ˆç›¸å…³ =====

use std::collections::HashMap;
use std::sync::atomic::{AtomicU64, Ordering};
use std::sync::LazyLock;

// å…¨å±€æµIDè®¡æ•°å™¨
static STREAM_COUNTER: AtomicU64 = AtomicU64::new(0);

// æµçŠ¶æ€ç®¡ç†
static STREAMS: LazyLock<Mutex<HashMap<String, DirectoryStream>>> =
    LazyLock::new(|| Mutex::new(HashMap::new()));

#[derive(Debug)]
struct DirectoryStream {
    id: String,
    path: PathBuf,
    entries: Vec<PathBuf>, // æ”¹ä¸ºå­˜å‚¨PathBufè€Œä¸æ˜¯DirEntry
    current_index: usize,
    batch_size: usize,
    total: usize,
}

#[derive(Debug, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct DirectoryPageResult {
    pub items: Vec<FileInfo>,
    pub total: usize,
    pub has_more: bool,
    pub next_offset: Option<usize>,
}

#[derive(Debug, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct DirectoryStreamStartResult {
    pub stream_id: String,
    pub initial_batch: Vec<FileInfo>,
    pub total: usize,
    pub has_more: bool,
}

#[derive(Debug, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct StreamBatchResult {
    pub items: Vec<FileInfo>,
    pub has_more: bool,
}

/// åˆ†é¡µæµè§ˆç›®å½•
#[tauri::command]
pub async fn browse_directory_page(
    path: String,
    options: Option<DirectoryPageOptions>,
) -> Result<DirectoryPageResult, String> {
    let options = options.unwrap_or_default();
    let path = Path::new(&path);

    if !path.exists() {
        return Err(format!("Path does not exist: {}", path.display()));
    }

    if !path.is_dir() {
        return Err(format!("Path is not a directory: {}", path.display()));
    }

    // è¯»å–æ‰€æœ‰ç›®å½•æ¡ç›®
    let mut entries: Vec<PathBuf> = std::fs::read_dir(path)
        .map_err(|e| format!("Failed to read directory: {}", e))?
        .filter_map(Result::ok)
        .map(|entry| entry.path())
        .collect();

    // åº”ç”¨æ’åº
    sort_entries(&mut entries, &options.sort_by, &options.sort_order);

    let total = entries.len();
    let offset = options.offset.unwrap_or(0);
    let limit = options.limit.unwrap_or(100);

    // è·å–åˆ†é¡µæ•°æ®
    let page_entries: Vec<PathBuf> = entries.into_iter().skip(offset).take(limit).collect();

    // è½¬æ¢ä¸ºFileInfo
    let items = convert_paths_to_file_info(page_entries)?;

    let has_more = offset + items.len() < total;
    let next_offset = if has_more {
        Some(offset + items.len())
    } else {
        None
    };

    Ok(DirectoryPageResult {
        items,
        total,
        has_more,
        next_offset,
    })
}

/// å¯åŠ¨ç›®å½•æµ
#[tauri::command]
pub async fn start_directory_stream(
    path: String,
    options: Option<DirectoryStreamOptions>,
) -> Result<DirectoryStreamStartResult, String> {
    let options = options.unwrap_or_default();
    let path = Path::new(&path);

    if !path.exists() {
        return Err(format!("Path does not exist: {}", path.display()));
    }

    if !path.is_dir() {
        return Err(format!("Path is not a directory: {}", path.display()));
    }

    // è¯»å–æ‰€æœ‰ç›®å½•æ¡ç›®
    let mut entries: Vec<PathBuf> = std::fs::read_dir(path)
        .map_err(|e| format!("Failed to read directory: {}", e))?
        .filter_map(Result::ok)
        .map(|entry| entry.path())
        .collect();

    // åº”ç”¨æ’åº
    sort_entries(&mut entries, &options.sort_by, &options.sort_order);

    let total = entries.len();
    let batch_size = options.batch_size.unwrap_or(50);
    let stream_id = format!("stream_{}", STREAM_COUNTER.fetch_add(1, Ordering::SeqCst));

    // è·å–åˆå§‹æ‰¹æ¬¡
    let initial_batch: Vec<PathBuf> = entries.iter().take(batch_size).cloned().collect();

    let initial_items = convert_paths_to_file_info(initial_batch)?;
    let has_more = batch_size < total;

    // åˆ›å»ºæµçŠ¶æ€
    let stream = DirectoryStream {
        id: stream_id.clone(),
        path: path.to_path_buf(),
        entries,
        current_index: batch_size,
        batch_size,
        total,
    };

    // å­˜å‚¨æµçŠ¶æ€
    let mut streams = STREAMS.lock().unwrap();
    streams.insert(stream_id.clone(), stream);

    Ok(DirectoryStreamStartResult {
        stream_id,
        initial_batch: initial_items,
        total,
        has_more,
    })
}

/// è·å–æµçš„ä¸‹ä¸€æ‰¹æ•°æ®
#[tauri::command]
pub async fn get_next_stream_batch(stream_id: String) -> Result<StreamBatchResult, String> {
    let mut streams = STREAMS.lock().unwrap();

    if let Some(stream) = streams.get_mut(&stream_id) {
        if stream.current_index >= stream.entries.len() {
            // æ²¡æœ‰æ›´å¤šæ•°æ®
            return Ok(StreamBatchResult {
                items: vec![],
                has_more: false,
            });
        }

        // è·å–ä¸‹ä¸€æ‰¹
        let next_index = (stream.current_index + stream.batch_size).min(stream.entries.len());
        let batch: Vec<PathBuf> = stream.entries[stream.current_index..next_index]
            .iter()
            .cloned()
            .collect();

        stream.current_index = next_index;
        let has_more = stream.current_index < stream.entries.len();

        let items = convert_paths_to_file_info(batch)?;

        Ok(StreamBatchResult { items, has_more })
    } else {
        Err(format!("Stream not found: {}", stream_id))
    }
}

/// ç¼“å­˜ç´¢å¼•ç»Ÿè®¡
#[tauri::command]
pub async fn cache_index_stats(
    cache_index: State<'_, CacheIndexState>,
) -> Result<CacheIndexStats, String> {
    cache_index.db.stats()
}

/// è§¦å‘ç¼“å­˜ GC
#[tauri::command]
pub async fn cache_index_gc(
    cache_index: State<'_, CacheIndexState>,
) -> Result<CacheGcResult, String> {
    cache_index.db.run_gc()
}

/// å°†ç¼“å­˜ GC æäº¤åˆ°åå°è°ƒåº¦å™¨
#[tauri::command]
pub async fn enqueue_cache_maintenance(
    cache_index: State<'_, CacheIndexState>,
    scheduler: State<'_, BackgroundSchedulerState>,
) -> Result<CacheGcResult, String> {
    let db = Arc::clone(&cache_index.db);
    scheduler
        .scheduler
        .enqueue_blocking(
            "cache-maintenance",
            "cache_index_gc",
            move || -> Result<CacheGcResult, String> { db.run_gc().map_err(|e| e) },
        )
        .await
}

/// å–æ¶ˆç›®å½•æµ
#[tauri::command]
pub async fn cancel_directory_stream(stream_id: String) -> Result<(), String> {
    let mut streams = STREAMS.lock().unwrap();
    if streams.remove(&stream_id).is_some() {
        Ok(())
    } else {
        Err(format!("Stream not found: {}", stream_id))
    }
}

#[derive(Debug, Default, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct DirectoryPageOptions {
    pub offset: Option<usize>,
    pub limit: Option<usize>,
    pub sort_by: Option<String>,
    pub sort_order: Option<String>,
}

#[derive(Debug, Default, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct DirectoryStreamOptions {
    pub batch_size: Option<usize>,
    pub sort_by: Option<String>,
    pub sort_order: Option<String>,
}

fn sort_entries(entries: &mut Vec<PathBuf>, sort_by: &Option<String>, sort_order: &Option<String>) {
    let sort_by = sort_by.as_ref().map(|s| s.as_str()).unwrap_or("name");
    let sort_ascending = sort_order.as_ref().map(|s| s.as_str()).unwrap_or("asc") == "asc";

    entries.sort_by(|a, b| {
        let a_name = a.file_name().and_then(|n| n.to_str()).unwrap_or("");
        let b_name = b.file_name().and_then(|n| n.to_str()).unwrap_or("");

        let comparison = match sort_by {
            "name" => a_name.cmp(&b_name),
            "size" => {
                let a_size = a.metadata().ok().map(|m| m.len()).unwrap_or(0);
                let b_size = b.metadata().ok().map(|m| m.len()).unwrap_or(0);
                a_size.cmp(&b_size)
            }
            "modified" => {
                let a_modified = a.metadata().ok().and_then(|m| m.modified().ok());
                let b_modified = b.metadata().ok().and_then(|m| m.modified().ok());
                a_modified.cmp(&b_modified)
            }
            _ => a_name.cmp(&b_name),
        };

        if sort_ascending {
            comparison
        } else {
            comparison.reverse()
        }
    });
}

fn convert_paths_to_file_info(paths: Vec<PathBuf>) -> Result<Vec<FileInfo>, String> {
    let mut items = Vec::new();

    for path in paths {
        let metadata = std::fs::metadata(&path)
            .map_err(|e| format!("Failed to read metadata for {}: {}", path.display(), e))?;

        let name = path
            .file_name()
            .and_then(|n| n.to_str())
            .unwrap_or("")
            .to_string();

        let size = if metadata.is_file() {
            Some(metadata.len())
        } else {
            None
        };

        let modified = metadata
            .modified()
            .ok()
            .and_then(|t| t.elapsed().ok())
            .map(|duration| {
                let secs = duration.as_secs();
                format!("{} seconds ago", secs)
            });

        items.push(FileInfo {
            name,
            path: path.to_string_lossy().to_string(),
            is_directory: metadata.is_dir(),
            size,
            modified,
        });
    }

    Ok(items)
}
