//! NeoView - Job Engine
//! å‚è€ƒ NeeView çš„ JobEngineï¼Œå®ç°åç«¯ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ
//!
//! ## æ¶æ„
//!
//! ```text
//! JobEngine
//!   â”œâ”€â”€ JobScheduler (ä¼˜å…ˆçº§é˜Ÿåˆ—)
//!   â”œâ”€â”€ JobWorker[0] (Primary - é«˜ä¼˜å…ˆçº§)
//!   â”œâ”€â”€ JobWorker[1] (Primary - é«˜ä¼˜å…ˆçº§)
//!   â”œâ”€â”€ JobWorker[2] (Secondary - æ‰€æœ‰ä»»åŠ¡)
//!   â””â”€â”€ JobWorker[N] (Secondary - æ‰€æœ‰ä»»åŠ¡)
//! ```
//!
//! ## ä¼˜å…ˆçº§
//!
//! - `Urgent (100)`: ç´§æ€¥ä»»åŠ¡ï¼ˆåˆ‡ä¹¦ç­‰ï¼‰
//! - `CurrentPage (90)`: å½“å‰é¡µé¢åŠ è½½
//! - `Preload (50)`: é¢„åŠ è½½é¡µé¢
//! - `Thumbnail (10)`: ç¼©ç•¥å›¾åŠ è½½

mod job;
mod scheduler;
mod worker;

pub use job::{Job, JobCategory, JobError, JobOutput, JobPriority, JobResult};
pub use scheduler::{JobScheduler, SchedulerStats};
pub use worker::{JobCompletedEvent, JobWorker, WorkerConfig};

use std::sync::Arc;
use tokio::sync::{broadcast, mpsc, Mutex};

/// é»˜è®¤ Worker æ•°é‡
const DEFAULT_WORKER_COUNT: usize = 4;
/// Primary Worker æ•°é‡ï¼ˆå¤„ç†é«˜ä¼˜å…ˆçº§ä»»åŠ¡ï¼‰
const PRIMARY_WORKER_COUNT: usize = 2;
/// ç»“æœé€šé“ç¼“å†²åŒºå¤§å°
const RESULT_CHANNEL_SIZE: usize = 1024;

/// Job Engine é…ç½®
#[derive(Debug, Clone)]
pub struct JobEngineConfig {
    /// Worker æ€»æ•°
    pub worker_count: usize,
    /// Primary Worker æ•°é‡
    pub primary_count: usize,
}

impl Default for JobEngineConfig {
    fn default() -> Self {
        Self {
            worker_count: DEFAULT_WORKER_COUNT,
            primary_count: PRIMARY_WORKER_COUNT,
        }
    }
}

/// Job Engine ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone, serde::Serialize)]
#[serde(rename_all = "camelCase")]
pub struct JobEngineStats {
    /// è°ƒåº¦å™¨ç»Ÿè®¡
    pub scheduler: SchedulerStats,
    /// Worker æ•°é‡
    pub worker_count: usize,
    /// æ˜¯å¦æ­£åœ¨è¿è¡Œ
    pub is_running: bool,
}

/// Job Engine
///
/// ç®¡ç†ä»»åŠ¡é˜Ÿåˆ—å’Œå·¥ä½œçº¿ç¨‹æ± 
pub struct JobEngine {
    /// è°ƒåº¦å™¨
    scheduler: Arc<Mutex<JobScheduler>>,
    /// ç»“æœæ¥æ”¶å™¨
    result_rx: mpsc::Receiver<JobCompletedEvent>,
    /// å…³é—­ä¿¡å·å‘é€å™¨
    shutdown_tx: broadcast::Sender<()>,
    /// Worker å¥æŸ„
    worker_handles: Vec<tokio::task::JoinHandle<()>>,
    /// é…ç½®
    config: JobEngineConfig,
    /// æ˜¯å¦æ­£åœ¨è¿è¡Œ
    is_running: bool,
}

impl JobEngine {
    /// åˆ›å»ºæ–°çš„ Job Engine
    pub fn new(config: JobEngineConfig) -> Self {
        let scheduler = Arc::new(Mutex::new(JobScheduler::new()));
        let (result_tx, result_rx) = mpsc::channel(RESULT_CHANNEL_SIZE);
        let (shutdown_tx, _) = broadcast::channel(1);

        let mut worker_handles = Vec::with_capacity(config.worker_count);

        // åˆ›å»º Workers
        for i in 0..config.worker_count {
            let worker_config = if i < config.primary_count {
                WorkerConfig::primary(i)
            } else {
                WorkerConfig::secondary(i)
            };

            let worker = JobWorker::new(
                worker_config,
                Arc::clone(&scheduler),
                result_tx.clone(),
            );

            let shutdown_rx = shutdown_tx.subscribe();
            worker_handles.push(tokio::spawn(worker.run(shutdown_rx)));
        }

        log::info!(
            "ğŸš€ JobEngine å¯åŠ¨: {} workers ({} primary, {} secondary)",
            config.worker_count,
            config.primary_count,
            config.worker_count - config.primary_count
        );

        Self {
            scheduler,
            result_rx,
            shutdown_tx,
            worker_handles,
            config,
            is_running: true,
        }
    }

    /// ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»º
    pub fn with_defaults() -> Self {
        Self::new(JobEngineConfig::default())
    }

    /// æäº¤å•ä¸ªä»»åŠ¡
    pub async fn submit(&self, job: Job) -> tokio_util::sync::CancellationToken {
        let mut scheduler = self.scheduler.lock().await;
        scheduler.enqueue(job)
    }

    /// æ‰¹é‡æäº¤ä»»åŠ¡
    pub async fn submit_batch(&self, jobs: Vec<Job>) -> Vec<tokio_util::sync::CancellationToken> {
        let mut scheduler = self.scheduler.lock().await;
        scheduler.enqueue_batch(jobs)
    }

    /// å–æ¶ˆæŒ‡å®šä¹¦ç±çš„æ‰€æœ‰ä»»åŠ¡
    pub async fn cancel_book(&self, book_path: &str) {
        let mut scheduler = self.scheduler.lock().await;
        scheduler.cancel_by_prefix(&format!("page:{}:", book_path));
    }

    /// å–æ¶ˆæ‰€æœ‰ä»»åŠ¡
    pub async fn cancel_all(&self) {
        let mut scheduler = self.scheduler.lock().await;
        scheduler.cancel_all();
    }

    /// è·å–ç»“æœæ¥æ”¶å™¨çš„å¯å˜å¼•ç”¨
    pub fn result_receiver(&mut self) -> &mut mpsc::Receiver<JobCompletedEvent> {
        &mut self.result_rx
    }

    /// å°è¯•æ¥æ”¶ä¸‹ä¸€ä¸ªç»“æœï¼ˆéé˜»å¡ï¼‰
    pub fn try_recv_result(&mut self) -> Option<JobCompletedEvent> {
        self.result_rx.try_recv().ok()
    }

    /// è·å–ç»Ÿè®¡ä¿¡æ¯
    pub async fn stats(&self) -> JobEngineStats {
        let scheduler = self.scheduler.lock().await;
        JobEngineStats {
            scheduler: scheduler.stats(),
            worker_count: self.config.worker_count,
            is_running: self.is_running,
        }
    }

    /// æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
    pub async fn has_job(&self, key: &str) -> bool {
        let scheduler = self.scheduler.lock().await;
        scheduler.has_job(key)
    }

    /// å…³é—­å¼•æ“
    pub async fn shutdown(mut self) {
        if !self.is_running {
            return;
        }

        log::info!("ğŸ›‘ JobEngine æ­£åœ¨å…³é—­...");

        // å–æ¶ˆæ‰€æœ‰ä»»åŠ¡
        self.cancel_all().await;

        // å‘é€å…³é—­ä¿¡å·
        let _ = self.shutdown_tx.send(());

        // å”¤é†’æ‰€æœ‰ Worker
        {
            let scheduler = self.scheduler.lock().await;
            scheduler.wake_all();
        }

        // ç­‰å¾…æ‰€æœ‰ Worker å®Œæˆ
        let handles = std::mem::take(&mut self.worker_handles);
        for handle in handles {
            let _ = handle.await;
        }

        self.is_running = false;
        log::info!("ğŸ›‘ JobEngine å·²å…³é—­");
    }

    /// æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¿è¡Œ
    pub fn is_running(&self) -> bool {
        self.is_running
    }
}

impl Drop for JobEngine {
    fn drop(&mut self) {
        if self.is_running {
            // å‘é€å…³é—­ä¿¡å·ï¼ˆå¼‚æ­¥å…³é—­åœ¨ shutdown() ä¸­å¤„ç†ï¼‰
            let _ = self.shutdown_tx.send(());
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_engine_creation() {
        let engine = JobEngine::with_defaults();
        assert!(engine.is_running());

        let stats = engine.stats().await;
        assert_eq!(stats.worker_count, DEFAULT_WORKER_COUNT);

        engine.shutdown().await;
    }

    #[tokio::test]
    async fn test_submit_job() {
        let engine = JobEngine::with_defaults();

        let job = Job::new(
            "test:1".to_string(),
            JobPriority::CurrentPage,
            JobCategory::PageContent,
            |_token| async { Ok(JobOutput::Empty) },
        );

        let token = engine.submit(job).await;
        assert!(!token.is_cancelled());

        // ç­‰å¾…ä¸€ä¸‹è®©ä»»åŠ¡æ‰§è¡Œ
        tokio::time::sleep(std::time::Duration::from_millis(100)).await;

        engine.shutdown().await;
    }
}
