//! NeoView - Job Engine
//! å‚è€ƒ NeeView çš„ JobEngineï¼Œå®ç°åç«¯ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ
//!
//! ## æ¶æ„
//!
//! ```text
//! JobEngine
//!   â”œâ”€â”€ JobScheduler (ä¼˜å…ˆçº§é˜Ÿåˆ—)
//!   â”œâ”€â”€ JobWorker[0] (Primary - é«˜ä¼˜å…ˆçº§)
//!   â”œâ”€â”€ JobWorker[1] (Primary - é«˜ä¼˜å…ˆçº§)
//!   â”œâ”€â”€ JobWorker[2] (Secondary - æ‰€æœ‰ä»»åŠ¡)
//!   â””â”€â”€ JobWorker[N] (Secondary - æ‰€æœ‰ä»»åŠ¡)
//! ```
//!
//! ## ä¼˜å…ˆçº§
//!
//! - `Urgent (100)`: ç´§æ€¥ä»»åŠ¡ï¼ˆåˆ‡ä¹¦ç­‰ï¼‰
//! - `CurrentPage (90)`: å½“å‰é¡µé¢åŠ è½½
//! - `Preload (50)`: é¢„åŠ è½½é¡µé¢
//! - `Thumbnail (10)`: ç¼©ç•¥å›¾åŠ è½½

mod job;
mod scheduler;
mod worker;

pub use job::{Job, JobCategory, JobError, JobOutput, JobPriority, JobResult};
pub use scheduler::{JobScheduler, SchedulerStats};
pub use worker::{JobCompletedEvent, JobWorker, WorkerConfig};

use std::sync::Arc;
use tokio::sync::{broadcast, mpsc, Mutex};

/// é»˜è®¤ Worker æ•°é‡
const DEFAULT_WORKER_COUNT: usize = 4;
/// Primary Worker æ•°é‡ï¼ˆå¤„ç†é«˜ä¼˜å…ˆçº§ä»»åŠ¡ï¼‰
const PRIMARY_WORKER_COUNT: usize = 2;
/// ç»“æœé€šé“ç¼“å†²åŒºå¤§å°
const RESULT_CHANNEL_SIZE: usize = 1024;

/// Job Engine é…ç½®
#[derive(Debug, Clone)]
pub struct JobEngineConfig {
    /// Worker æ€»æ•°
    pub worker_count: usize,
    /// Primary Worker æ•°é‡
    pub primary_count: usize,
}

impl Default for JobEngineConfig {
    fn default() -> Self {
        Self {
            worker_count: DEFAULT_WORKER_COUNT,
            primary_count: PRIMARY_WORKER_COUNT,
        }
    }
}

/// Job Engine ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone, serde::Serialize)]
#[serde(rename_all = "camelCase")]
pub struct JobEngineStats {
    /// è°ƒåº¦å™¨ç»Ÿè®¡
    pub scheduler: SchedulerStats,
    /// Worker æ•°é‡
    pub worker_count: usize,
    /// æ˜¯å¦æ­£åœ¨è¿è¡Œ
    pub is_running: bool,
}

/// Worker å¯åŠ¨çŠ¶æ€ï¼ˆå†…éƒ¨å¯å˜æ€§ï¼‰
struct WorkerState {
    handles: Vec<tokio::task::JoinHandle<()>>,
    is_running: bool,
}

/// Job Engine
///
/// ç®¡ç†ä»»åŠ¡é˜Ÿåˆ—å’Œå·¥ä½œçº¿ç¨‹æ± 
pub struct JobEngine {
    /// è°ƒåº¦å™¨
    scheduler: Arc<Mutex<JobScheduler>>,
    /// ç»“æœæ¥æ”¶å™¨ï¼ˆæš‚æœªä½¿ç”¨ï¼‰
    #[allow(dead_code)]
    result_rx: std::sync::Mutex<Option<mpsc::Receiver<JobCompletedEvent>>>,
    /// ç»“æœå‘é€å™¨ï¼ˆç”¨äºåˆ›å»º workersï¼‰
    result_tx: mpsc::Sender<JobCompletedEvent>,
    /// å…³é—­ä¿¡å·å‘é€å™¨
    shutdown_tx: broadcast::Sender<()>,
    /// Worker çŠ¶æ€ï¼ˆä½¿ç”¨ std::sync::Mutex ä»¥ä¾¿åœ¨é async ä¸Šä¸‹æ–‡ä¿®æ”¹ï¼‰
    worker_state: std::sync::Mutex<WorkerState>,
    /// é…ç½®
    config: JobEngineConfig,
}

impl JobEngine {
    /// åˆ›å»ºæ–°çš„ Job Engineï¼ˆå»¶è¿Ÿå¯åŠ¨æ¨¡å¼ï¼‰
    /// 
    /// Workers ä¼šåœ¨é¦–æ¬¡æäº¤ä»»åŠ¡æ—¶è‡ªåŠ¨å¯åŠ¨
    pub fn new(config: JobEngineConfig) -> Self {
        let scheduler = Arc::new(Mutex::new(JobScheduler::new()));
        let (result_tx, result_rx) = mpsc::channel(RESULT_CHANNEL_SIZE);
        let (shutdown_tx, _) = broadcast::channel(1);

        Self {
            scheduler,
            result_rx: std::sync::Mutex::new(Some(result_rx)),
            result_tx,
            shutdown_tx,
            worker_state: std::sync::Mutex::new(WorkerState {
                handles: Vec::new(),
                is_running: false,
            }),
            config,
        }
    }

    /// ç¡®ä¿ Workers å·²å¯åŠ¨ï¼ˆåœ¨ tokio runtime ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨ï¼‰
    fn ensure_started(&self) {
        let mut state = self.worker_state.lock().unwrap();
        if state.is_running {
            return;
        }

        // åˆ›å»º Workers
        for i in 0..self.config.worker_count {
            let worker_config = if i < self.config.primary_count {
                WorkerConfig::primary(i)
            } else {
                WorkerConfig::secondary(i)
            };

            let worker = JobWorker::new(
                worker_config,
                Arc::clone(&self.scheduler),
                self.result_tx.clone(),
            );

            let shutdown_rx = self.shutdown_tx.subscribe();
            state.handles.push(tokio::spawn(worker.run(shutdown_rx)));
        }

        log::info!(
            "ğŸš€ JobEngine å¯åŠ¨: {} workers ({} primary, {} secondary)",
            self.config.worker_count,
            self.config.primary_count,
            self.config.worker_count - self.config.primary_count
        );

        state.is_running = true;
    }

    /// ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»º
    pub fn with_defaults() -> Self {
        Self::new(JobEngineConfig::default())
    }

    /// æäº¤å•ä¸ªä»»åŠ¡
    pub async fn submit(&self, job: Job) -> tokio_util::sync::CancellationToken {
        self.ensure_started();
        let mut scheduler = self.scheduler.lock().await;
        scheduler.enqueue(job)
    }

    /// æ‰¹é‡æäº¤ä»»åŠ¡
    pub async fn submit_batch(&self, jobs: Vec<Job>) -> Vec<tokio_util::sync::CancellationToken> {
        self.ensure_started();
        let mut scheduler = self.scheduler.lock().await;
        scheduler.enqueue_batch(jobs)
    }

    /// å–æ¶ˆæŒ‡å®šä¹¦ç±çš„æ‰€æœ‰ä»»åŠ¡
    pub async fn cancel_book(&self, book_path: &str) {
        let mut scheduler = self.scheduler.lock().await;
        scheduler.cancel_by_prefix(&format!("page:{}:", book_path));
    }

    /// å–æ¶ˆæ‰€æœ‰ä»»åŠ¡
    pub async fn cancel_all(&self) {
        let mut scheduler = self.scheduler.lock().await;
        scheduler.cancel_all();
    }

    /// è·å–ç»Ÿè®¡ä¿¡æ¯
    pub async fn stats(&self) -> JobEngineStats {
        let scheduler = self.scheduler.lock().await;
        let state = self.worker_state.lock().unwrap();
        JobEngineStats {
            scheduler: scheduler.stats(),
            worker_count: self.config.worker_count,
            is_running: state.is_running,
        }
    }

    /// æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
    pub async fn has_job(&self, key: &str) -> bool {
        let scheduler = self.scheduler.lock().await;
        scheduler.has_job(key)
    }

    /// å…³é—­å¼•æ“
    pub async fn shutdown(&self) {
        let is_running = {
            let state = self.worker_state.lock().unwrap();
            state.is_running
        };
        
        if !is_running {
            return;
        }

        log::info!("ğŸ›‘ JobEngine æ­£åœ¨å…³é—­...");

        // å–æ¶ˆæ‰€æœ‰ä»»åŠ¡
        self.cancel_all().await;

        // å‘é€å…³é—­ä¿¡å·
        let _ = self.shutdown_tx.send(());

        // å”¤é†’æ‰€æœ‰ Worker
        {
            let scheduler = self.scheduler.lock().await;
            scheduler.wake_all();
        }

        // ç­‰å¾…æ‰€æœ‰ Worker å®Œæˆ
        let handles = {
            let mut state = self.worker_state.lock().unwrap();
            state.is_running = false;
            std::mem::take(&mut state.handles)
        };
        
        for handle in handles {
            let _ = handle.await;
        }

        log::info!("ğŸ›‘ JobEngine å·²å…³é—­");
    }

    /// æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¿è¡Œ
    pub fn is_running(&self) -> bool {
        let state = self.worker_state.lock().unwrap();
        state.is_running
    }
}

impl Drop for JobEngine {
    fn drop(&mut self) {
        let is_running = {
            let state = self.worker_state.lock().unwrap();
            state.is_running
        };
        if is_running {
            // å‘é€å…³é—­ä¿¡å·ï¼ˆå¼‚æ­¥å…³é—­åœ¨ shutdown() ä¸­å¤„ç†ï¼‰
            let _ = self.shutdown_tx.send(());
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_engine_creation() {
        let engine = JobEngine::with_defaults();
        engine.ensure_started();
        assert!(engine.is_running());

        let stats = engine.stats().await;
        assert_eq!(stats.worker_count, DEFAULT_WORKER_COUNT);

        engine.shutdown().await;
    }

    #[tokio::test]
    async fn test_submit_job() {
        let engine = JobEngine::with_defaults();

        let job = Job::new(
            "test:1".to_string(),
            JobPriority::CurrentPage,
            JobCategory::PageContent,
            |_token| async { Ok(JobOutput::Empty) },
        );

        let token = engine.submit(job).await;
        assert!(!token.is_cancelled());

        // ç­‰å¾…ä¸€ä¸‹è®©ä»»åŠ¡æ‰§è¡Œ
        tokio::time::sleep(std::time::Duration::from_millis(100)).await;

        engine.shutdown().await;
    }
}
