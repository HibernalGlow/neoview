//! æµå¼ä¼ è¾“ç®¡ç†å™¨
//!
//! ä¸ºå¤§æ–‡ä»¶æä¾›åˆ†å—æµå¼ä¼ è¾“ï¼Œæ”¯æŒè¿›åº¦åé¦ˆå’Œå–æ¶ˆ

use log::{debug, info, warn};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering};
use std::sync::{Arc, RwLock};
use std::time::Instant;

/// é»˜è®¤å—å¤§å° (256KB)
pub const DEFAULT_CHUNK_SIZE: usize = 256 * 1024;

/// å¤§æ–‡ä»¶é˜ˆå€¼ (1MB)
pub const LARGE_FILE_THRESHOLD: usize = 1024 * 1024;

/// é»˜è®¤æœ€å¤§å¹¶å‘ä¼ è¾“æ•°
pub const DEFAULT_MAX_CONCURRENT: usize = 4;

/// ä¼ è¾“å—
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct TransferChunk {
    /// ä¼ è¾“ ID
    pub transfer_id: String,
    /// å—ç´¢å¼•
    pub chunk_index: usize,
    /// æ€»å—æ•°
    pub total_chunks: usize,
    /// å—æ•°æ® (Base64 ç¼–ç )
    pub data: String,
    /// æ˜¯å¦ä¸ºæœ€åä¸€å—
    pub is_last: bool,
    /// å—å¤§å°
    pub chunk_size: usize,
}

/// ä¼ è¾“è¿›åº¦
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct TransferProgress {
    /// ä¼ è¾“ ID
    pub transfer_id: String,
    /// å·²ä¼ è¾“å­—èŠ‚æ•°
    pub transferred: usize,
    /// æ€»å­—èŠ‚æ•°
    pub total: usize,
    /// è¿›åº¦ç™¾åˆ†æ¯” (0-100)
    pub percentage: f64,
    /// ä¼ è¾“é€Ÿåº¦ï¼ˆå­—èŠ‚/ç§’ï¼‰
    pub speed: f64,
    /// é¢„è®¡å‰©ä½™æ—¶é—´ï¼ˆç§’ï¼‰
    pub eta: f64,
}

/// ä¼ è¾“çŠ¶æ€
#[derive(Debug)]
pub struct TransferState {
    /// ä¼ è¾“ ID
    pub id: String,
    /// å¼€å§‹æ—¶é—´
    pub started_at: Instant,
    /// æ€»å¤§å°
    pub total_size: usize,
    /// å·²ä¼ è¾“å¤§å°
    pub transferred: AtomicUsize,
    /// æ˜¯å¦å·²å–æ¶ˆ
    pub cancelled: AtomicBool,
    /// é‡è¯•æ¬¡æ•°
    pub retry_count: AtomicUsize,
    /// æœ€åæ›´æ–°æ—¶é—´
    pub last_update: RwLock<Instant>,
}

impl TransferState {
    /// åˆ›å»ºæ–°çš„ä¼ è¾“çŠ¶æ€
    pub fn new(id: String, total_size: usize) -> Self {
        Self {
            id,
            started_at: Instant::now(),
            total_size,
            transferred: AtomicUsize::new(0),
            cancelled: AtomicBool::new(false),
            retry_count: AtomicUsize::new(0),
            last_update: RwLock::new(Instant::now()),
        }
    }

    /// æ›´æ–°å·²ä¼ è¾“å¤§å°
    pub fn update_transferred(&self, bytes: usize) {
        self.transferred.fetch_add(bytes, Ordering::Relaxed);
        if let Ok(mut last) = self.last_update.write() {
            *last = Instant::now();
        }
    }

    /// è·å–è¿›åº¦
    pub fn get_progress(&self) -> TransferProgress {
        let transferred = self.transferred.load(Ordering::Relaxed);
        let elapsed = self.started_at.elapsed().as_secs_f64();
        let speed = if elapsed > 0.0 {
            transferred as f64 / elapsed
        } else {
            0.0
        };
        let remaining = self.total_size.saturating_sub(transferred);
        let eta = if speed > 0.0 {
            remaining as f64 / speed
        } else {
            0.0
        };
        let percentage = if self.total_size > 0 {
            (transferred as f64 / self.total_size as f64) * 100.0
        } else {
            0.0
        };

        TransferProgress {
            transfer_id: self.id.clone(),
            transferred,
            total: self.total_size,
            percentage,
            speed,
            eta,
        }
    }

    /// æ˜¯å¦å·²å–æ¶ˆ
    pub fn is_cancelled(&self) -> bool {
        self.cancelled.load(Ordering::Relaxed)
    }

    /// å–æ¶ˆä¼ è¾“
    pub fn cancel(&self) {
        self.cancelled.store(true, Ordering::Relaxed);
    }
}

/// æµå¼ä¼ è¾“ç®¡ç†å™¨ç»Ÿè®¡
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct StreamTransferStats {
    /// æ´»åŠ¨ä¼ è¾“æ•°
    pub active_transfers: usize,
    /// æ€»ä¼ è¾“æ•°
    pub total_transfers: u64,
    /// æ€»ä¼ è¾“å­—èŠ‚æ•°
    pub total_bytes: u64,
    /// æœ€å¤§å¹¶å‘æ•°
    pub max_concurrent: usize,
    /// å—å¤§å°
    pub chunk_size: usize,
    /// å¤§æ–‡ä»¶é˜ˆå€¼
    pub large_file_threshold: usize,
}

/// æµå¼ä¼ è¾“ç®¡ç†å™¨
pub struct StreamTransferManager {
    /// æ´»åŠ¨ä¼ è¾“
    active_transfers: RwLock<HashMap<String, Arc<TransferState>>>,
    /// æœ€å¤§å¹¶å‘ä¼ è¾“æ•°
    max_concurrent: usize,
    /// å—å¤§å°
    chunk_size: usize,
    /// å¤§æ–‡ä»¶é˜ˆå€¼
    large_file_threshold: usize,
    /// æ€»ä¼ è¾“æ•°
    total_transfers: AtomicUsize,
    /// æ€»ä¼ è¾“å­—èŠ‚æ•°
    total_bytes: AtomicUsize,
}

impl StreamTransferManager {
    /// åˆ›å»ºæµå¼ä¼ è¾“ç®¡ç†å™¨
    pub fn new() -> Self {
        Self {
            active_transfers: RwLock::new(HashMap::new()),
            max_concurrent: DEFAULT_MAX_CONCURRENT,
            chunk_size: DEFAULT_CHUNK_SIZE,
            large_file_threshold: LARGE_FILE_THRESHOLD,
            total_transfers: AtomicUsize::new(0),
            total_bytes: AtomicUsize::new(0),
        }
    }

    /// åˆ›å»ºå¸¦è‡ªå®šä¹‰é…ç½®çš„ç®¡ç†å™¨
    pub fn with_config(max_concurrent: usize, chunk_size: usize, large_file_threshold: usize) -> Self {
        Self {
            active_transfers: RwLock::new(HashMap::new()),
            max_concurrent,
            chunk_size,
            large_file_threshold,
            total_transfers: AtomicUsize::new(0),
            total_bytes: AtomicUsize::new(0),
        }
    }

    /// æ˜¯å¦åº”ä½¿ç”¨æµå¼ä¼ è¾“
    pub fn should_stream(&self, size: usize) -> bool {
        size > self.large_file_threshold
    }

    /// å¼€å§‹ä¼ è¾“
    pub fn start_transfer(&self, transfer_id: &str, total_size: usize) -> Result<Arc<TransferState>, String> {
        // æ£€æŸ¥å¹¶å‘é™åˆ¶
        let active_count = self.active_transfers.read()
            .map(|t| t.len())
            .unwrap_or(0);
        
        if active_count >= self.max_concurrent {
            return Err(format!(
                "å·²è¾¾åˆ°æœ€å¤§å¹¶å‘ä¼ è¾“æ•° ({}), è¯·ç¨åé‡è¯•",
                self.max_concurrent
            ));
        }

        let state = Arc::new(TransferState::new(transfer_id.to_string(), total_size));
        
        if let Ok(mut transfers) = self.active_transfers.write() {
            transfers.insert(transfer_id.to_string(), state.clone());
        }

        self.total_transfers.fetch_add(1, Ordering::Relaxed);
        info!("ğŸ“¤ å¼€å§‹æµå¼ä¼ è¾“: id={} size={}", transfer_id, total_size);

        Ok(state)
    }

    /// å®Œæˆä¼ è¾“
    pub fn complete_transfer(&self, transfer_id: &str) {
        if let Ok(mut transfers) = self.active_transfers.write() {
            if let Some(state) = transfers.remove(transfer_id) {
                let transferred = state.transferred.load(Ordering::Relaxed);
                self.total_bytes.fetch_add(transferred, Ordering::Relaxed);
                info!(
                    "âœ… æµå¼ä¼ è¾“å®Œæˆ: id={} bytes={} elapsed={}ms",
                    transfer_id,
                    transferred,
                    state.started_at.elapsed().as_millis()
                );
            }
        }
    }

    /// å–æ¶ˆä¼ è¾“
    pub fn cancel_transfer(&self, transfer_id: &str) -> bool {
        if let Ok(transfers) = self.active_transfers.read() {
            if let Some(state) = transfers.get(transfer_id) {
                state.cancel();
                info!("âŒ æµå¼ä¼ è¾“å·²å–æ¶ˆ: id={}", transfer_id);
                return true;
            }
        }
        false
    }

    /// è·å–ä¼ è¾“è¿›åº¦
    pub fn get_progress(&self, transfer_id: &str) -> Option<TransferProgress> {
        self.active_transfers.read().ok()
            .and_then(|t| t.get(transfer_id).map(|s| s.get_progress()))
    }

    /// è·å–ä¼ è¾“çŠ¶æ€
    pub fn get_state(&self, transfer_id: &str) -> Option<Arc<TransferState>> {
        self.active_transfers.read().ok()
            .and_then(|t| t.get(transfer_id).cloned())
    }

    /// å°†æ•°æ®åˆ†å—
    pub fn chunk_data(&self, data: &[u8]) -> Vec<Vec<u8>> {
        data.chunks(self.chunk_size)
            .map(|chunk| chunk.to_vec())
            .collect()
    }

    /// ç”Ÿæˆä¼ è¾“å—
    pub fn create_chunks(&self, transfer_id: &str, data: &[u8]) -> Vec<TransferChunk> {
        let chunks: Vec<Vec<u8>> = self.chunk_data(data);
        let total_chunks = chunks.len();

        chunks.into_iter().enumerate().map(|(i, chunk)| {
            use base64::{Engine as _, engine::general_purpose::STANDARD};
            let chunk_size = chunk.len();
            TransferChunk {
                transfer_id: transfer_id.to_string(),
                chunk_index: i,
                total_chunks,
                data: STANDARD.encode(&chunk),
                is_last: i == total_chunks - 1,
                chunk_size,
            }
        }).collect()
    }

    /// è·å–ç»Ÿè®¡ä¿¡æ¯
    pub fn stats(&self) -> StreamTransferStats {
        let active_transfers = self.active_transfers.read()
            .map(|t| t.len())
            .unwrap_or(0);

        StreamTransferStats {
            active_transfers,
            total_transfers: self.total_transfers.load(Ordering::Relaxed) as u64,
            total_bytes: self.total_bytes.load(Ordering::Relaxed) as u64,
            max_concurrent: self.max_concurrent,
            chunk_size: self.chunk_size,
            large_file_threshold: self.large_file_threshold,
        }
    }

    /// æ¸…ç†å·²å®Œæˆæˆ–å·²å–æ¶ˆçš„ä¼ è¾“
    pub fn cleanup(&self) {
        if let Ok(mut transfers) = self.active_transfers.write() {
            transfers.retain(|_, state| !state.is_cancelled());
        }
    }
}

impl Default for StreamTransferManager {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_transfer_state() {
        let state = TransferState::new("test-1".to_string(), 1024);
        assert_eq!(state.total_size, 1024);
        assert!(!state.is_cancelled());
        
        state.update_transferred(512);
        let progress = state.get_progress();
        assert_eq!(progress.transferred, 512);
        assert_eq!(progress.total, 1024);
        assert!((progress.percentage - 50.0).abs() < 0.1);
    }

    #[test]
    fn test_stream_transfer_manager() {
        let manager = StreamTransferManager::new();
        
        // æµ‹è¯•æ˜¯å¦åº”è¯¥æµå¼ä¼ è¾“
        assert!(!manager.should_stream(512 * 1024)); // 512KB
        assert!(manager.should_stream(2 * 1024 * 1024)); // 2MB
        
        // æµ‹è¯•å¼€å§‹ä¼ è¾“
        let state = manager.start_transfer("test-1", 1024 * 1024).unwrap();
        assert_eq!(state.total_size, 1024 * 1024);
        
        // æµ‹è¯•ç»Ÿè®¡
        let stats = manager.stats();
        assert_eq!(stats.active_transfers, 1);
        
        // æµ‹è¯•å–æ¶ˆ
        assert!(manager.cancel_transfer("test-1"));
        assert!(state.is_cancelled());
    }

    #[test]
    fn test_chunk_data() {
        let manager = StreamTransferManager::with_config(4, 100, 1024);
        let data = vec![0u8; 250];
        let chunks = manager.chunk_data(&data);
        
        assert_eq!(chunks.len(), 3);
        assert_eq!(chunks[0].len(), 100);
        assert_eq!(chunks[1].len(), 100);
        assert_eq!(chunks[2].len(), 50);
    }

    #[test]
    fn test_concurrent_limit() {
        let manager = StreamTransferManager::with_config(2, 1024, 1024);
        
        // å¼€å§‹ä¸¤ä¸ªä¼ è¾“
        manager.start_transfer("test-1", 1024).unwrap();
        manager.start_transfer("test-2", 1024).unwrap();
        
        // ç¬¬ä¸‰ä¸ªåº”è¯¥å¤±è´¥
        let result = manager.start_transfer("test-3", 1024);
        assert!(result.is_err());
    }
}


// ============================================================================
// Property-Based Tests
// ============================================================================

#[cfg(test)]
mod property_tests {
    use super::*;
    use proptest::prelude::*;

    proptest! {
        /// **Feature: archive-ipc-optimization, Property 5: Streaming transfer data integrity (round-trip)**
        /// *For any* binary data transferred via streaming, assembling all chunks
        /// SHALL produce data identical to the original.
        /// **Validates: Requirements 3.3**
        #[test]
        fn prop_streaming_data_integrity(
            data in prop::collection::vec(any::<u8>(), 0..10000)
        ) {
            use base64::{Engine as _, engine::general_purpose::STANDARD};
            
            let manager = StreamTransferManager::with_config(4, 100, 50);
            let chunks = manager.create_chunks("test", &data);
            
            // é‡æ–°ç»„è£…æ•°æ®
            let mut reassembled = Vec::new();
            for chunk in &chunks {
                let decoded = STANDARD.decode(&chunk.data).unwrap();
                reassembled.extend(decoded);
            }
            
            // éªŒè¯æ•°æ®å®Œæ•´æ€§
            prop_assert_eq!(reassembled, data);
        }

        /// **Feature: archive-ipc-optimization, Property 6: Progress reporting monotonicity**
        /// *For any* streaming transfer operation, progress percentage SHALL
        /// monotonically increase from 0 to 100.
        /// **Validates: Requirements 2.1, 3.2**
        #[test]
        fn prop_progress_monotonicity(
            updates in prop::collection::vec(1usize..1000, 1..20)
        ) {
            let total: usize = updates.iter().sum();
            let state = TransferState::new("test".to_string(), total);
            
            let mut last_percentage = 0.0;
            let mut transferred = 0usize;
            
            for update in updates {
                transferred += update;
                state.transferred.store(transferred, Ordering::Relaxed);
                
                let progress = state.get_progress();
                
                // è¿›åº¦åº”è¯¥å•è°ƒé€’å¢
                prop_assert!(progress.percentage >= last_percentage);
                last_percentage = progress.percentage;
            }
            
            // æœ€ç»ˆè¿›åº¦åº”è¯¥æ˜¯ 100%
            prop_assert!((last_percentage - 100.0).abs() < 0.01);
        }

        /// **Feature: archive-ipc-optimization, Property 7: Concurrent transfer limiting**
        /// *For any* number of concurrent transfer requests, the system SHALL not
        /// exceed the configured maximum concurrent transfers.
        /// **Validates: Requirements 5.2**
        #[test]
        fn prop_concurrent_transfer_limiting(
            max_concurrent in 1usize..10,
            request_count in 1usize..20
        ) {
            let manager = StreamTransferManager::with_config(max_concurrent, 1024, 1024);
            
            let mut successful = 0;
            for i in 0..request_count {
                if manager.start_transfer(&format!("test-{}", i), 1024).is_ok() {
                    successful += 1;
                }
            }
            
            // æˆåŠŸçš„ä¼ è¾“æ•°ä¸åº”è¶…è¿‡æœ€å¤§å¹¶å‘æ•°
            prop_assert!(successful <= max_concurrent);
            
            // æ´»åŠ¨ä¼ è¾“æ•°ä¸åº”è¶…è¿‡æœ€å¤§å¹¶å‘æ•°
            let stats = manager.stats();
            prop_assert!(stats.active_transfers <= max_concurrent);
        }
    }
}
