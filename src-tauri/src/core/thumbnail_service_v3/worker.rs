//! å·¥ä½œçº¿ç¨‹æ¨¡å—
//! åŒ…å«å·¥ä½œçº¿ç¨‹å¯åŠ¨é€»è¾‘ã€ä»»åŠ¡å¤„ç†å¾ªç¯ã€ä¿å­˜é˜Ÿåˆ—åˆ·æ–°çº¿ç¨‹

use lru::LruCache;
use std::collections::{HashMap, HashSet};
use std::panic;
use std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};
use std::sync::{Arc, Condvar, Mutex, RwLock};
use std::thread::{self, JoinHandle};
use std::time::{Duration, Instant};
use tauri::{AppHandle, Emitter};

use crate::core::thumbnail_db::ThumbnailDb;
use crate::core::thumbnail_generator::ThumbnailGenerator;
use crate::core::request_dedup::RequestDeduplicator;

use super::config::{LaneQuota, ThumbnailServiceConfig};
use super::generators::{
    generate_archive_thumbnail_static, generate_file_thumbnail_static,
    generate_folder_thumbnail_static, generate_video_thumbnail_static,
};
use super::queue;
use super::types::{
    GenerateTask, TaskLane, ThumbnailBatchReadyPayload, ThumbnailFileType, ThumbnailReadyPayload,
};
use super::{log_debug, log_info};

fn lane_from_quota_tick(tick: usize, quota: LaneQuota) -> TaskLane {
    let visible_slots = quota.visible;
    let prefetch_slots = quota.prefetch;
    let background_slots = quota.background;
    let total = visible_slots + prefetch_slots + background_slots;

    if total == 0 {
        return TaskLane::Visible;
    }

    let slot = tick % total;
    if slot < visible_slots {
        return TaskLane::Visible;
    }
    if slot < visible_slots + prefetch_slots {
        return TaskLane::Prefetch;
    }
    TaskLane::Background
}

fn backlog_is_empty(
    queued_visible: &Arc<AtomicUsize>,
    queued_prefetch: &Arc<AtomicUsize>,
    queued_background: &Arc<AtomicUsize>,
) -> bool {
    queued_visible.load(Ordering::Relaxed)
        + queued_prefetch.load(Ordering::Relaxed)
        + queued_background.load(Ordering::Relaxed)
        == 0
}

fn preferred_lane_for_tick(
    tick: usize,
    visible: usize,
    prefetch: usize,
    background: usize,
    visible_boost_factor: usize,
    side_boost_factor: usize,
    visible_boost_quota: LaneQuota,
    default_quota: LaneQuota,
    side_boost_quota: LaneQuota,
) -> TaskLane {
    let side_total = prefetch + background;

    // visible ç§¯å‹æ˜æ˜¾æ—¶ï¼Œæå‡å‰å°é…é¢ï¼ˆ8:1:1ï¼‰
    if visible > 0 && visible >= side_total.saturating_mul(visible_boost_factor.max(1)) {
        return lane_from_quota_tick(tick, visible_boost_quota);
    }

    // åå°/é¢„å–ç§¯å‹æ˜æ˜¾æ—¶ï¼Œæ”¾å®½ä¸º 4:3:3 æå‡æ€»ä½“åå
    if side_total > visible.saturating_mul(side_boost_factor.max(1)) {
        return lane_from_quota_tick(tick, side_boost_quota);
    }

    // é»˜è®¤ 6:2:1 æ—¶é—´ç‰‡é…é¢ï¼ˆvisible:prefetch:backgroundï¼‰
    lane_from_quota_tick(tick, default_quota)
}

fn needs_decode_limit(task: &GenerateTask) -> bool {
    matches!(
        task.file_type,
        ThumbnailFileType::Archive | ThumbnailFileType::Video | ThumbnailFileType::Folder
    )
}

fn needs_encode_limit(task: &GenerateTask) -> bool {
    matches!(
        task.file_type,
        ThumbnailFileType::Archive
            | ThumbnailFileType::Video
            | ThumbnailFileType::Image
            | ThumbnailFileType::Other
    )
}

fn needs_scale_limit(task: &GenerateTask) -> bool {
    matches!(
        task.file_type,
        ThumbnailFileType::Archive
            | ThumbnailFileType::Video
            | ThumbnailFileType::Image
            | ThumbnailFileType::Other
    )
}

#[derive(Default)]
struct AdaptiveStats {
    completed: usize,
    failed: usize,
    total_elapsed_ms: u128,
}

fn start_adaptive_control_thread(
    config: ThumbnailServiceConfig,
    running: Arc<AtomicBool>,
    queued_visible: Arc<AtomicUsize>,
    queued_prefetch: Arc<AtomicUsize>,
    queued_background: Arc<AtomicUsize>,
    worker_budget: Arc<AtomicUsize>,
    adaptive_completed: Arc<AtomicUsize>,
    adaptive_failed: Arc<AtomicUsize>,
    adaptive_total_elapsed_ms: Arc<AtomicU64>,
) -> JoinHandle<()> {
    thread::spawn(move || {
        while running.load(Ordering::SeqCst) {
            thread::sleep(Duration::from_millis(config.adaptive_tick_ms.max(100)));

            if !running.load(Ordering::SeqCst) {
                break;
            }

            let completed = adaptive_completed.swap(0, Ordering::AcqRel);
            let failed = adaptive_failed.swap(0, Ordering::AcqRel);
            let elapsed = adaptive_total_elapsed_ms.swap(0, Ordering::AcqRel);
            let backlog = queued_visible.load(Ordering::Relaxed)
                + queued_prefetch.load(Ordering::Relaxed)
                + queued_background.load(Ordering::Relaxed);

            if completed == 0 {
                if backlog > config.adaptive_scale_up_backlog {
                    let current = worker_budget.load(Ordering::Relaxed);
                    if current < config.worker_threads {
                        worker_budget.store(current + 1, Ordering::Relaxed);
                    }
                }
                continue;
            }

            let avg_ms = elapsed / completed as u64;
            let fail_percent = (failed * 100) / completed.max(1);
            let current = worker_budget.load(Ordering::Relaxed);

            let should_scale_down = avg_ms >= config.adaptive_scale_down_avg_ms
                || fail_percent >= config.adaptive_scale_down_fail_percent;
            let should_scale_up = backlog >= config.adaptive_scale_up_backlog
                && avg_ms <= config.adaptive_scale_up_avg_ms
                && fail_percent <= config.adaptive_scale_down_fail_percent / 2;

            if should_scale_down && current > config.adaptive_min_active_workers {
                worker_budget.store(current - 1, Ordering::Relaxed);
            } else if should_scale_up && current < config.worker_threads {
                worker_budget.store(current + 1, Ordering::Relaxed);
            }
        }
    })
}

/// å¯åŠ¨å·¥ä½œçº¿ç¨‹
#[allow(clippy::too_many_arguments)]
pub fn start_workers(
    config: &ThumbnailServiceConfig,
    running: Arc<AtomicBool>,
    task_queue: Arc<(Mutex<queue::TaskQueueState>, Condvar)>,
    current_dir: Arc<RwLock<String>>,
    request_epoch: Arc<AtomicU64>,
    scheduler_paused: Arc<AtomicBool>,
    active_workers: Arc<AtomicUsize>,
    queued_visible: Arc<AtomicUsize>,
    queued_prefetch: Arc<AtomicUsize>,
    queued_background: Arc<AtomicUsize>,
    processed_visible: Arc<AtomicUsize>,
    processed_prefetch: Arc<AtomicUsize>,
    processed_background: Arc<AtomicUsize>,
    decode_wait_count: Arc<AtomicUsize>,
    decode_wait_ms: Arc<AtomicU64>,
    scale_wait_count: Arc<AtomicUsize>,
    scale_wait_ms: Arc<AtomicU64>,
    encode_wait_count: Arc<AtomicUsize>,
    encode_wait_ms: Arc<AtomicU64>,
    memory_cache: Arc<RwLock<LruCache<String, Vec<u8>>>>,
    memory_cache_bytes: Arc<AtomicUsize>,
    db: Arc<ThumbnailDb>,
    generator: Arc<ThumbnailGenerator>,
    db_index: Arc<RwLock<HashSet<String>>>,
    folder_db_index: Arc<RwLock<HashSet<String>>>,
    failed_index: Arc<RwLock<HashSet<String>>>,
    save_queue: Arc<Mutex<HashMap<String, (Vec<u8>, i64, i32, Instant)>>>,
    request_deduplicator: Arc<RequestDeduplicator>,
    app: AppHandle,
) -> Vec<JoinHandle<()>> {
    let mut workers = Vec::new();
    let visible_boost_factor = config.scheduler_visible_boost_factor;
    let side_boost_factor = config.scheduler_side_boost_factor;
    let visible_boost_quota = config.scheduler_visible_boost_quota;
    let default_quota = config.scheduler_default_quota;
    let side_boost_quota = config.scheduler_side_boost_quota;
    let worker_budget = Arc::new(AtomicUsize::new(
        config
            .adaptive_min_active_workers
            .max(1)
            .min(config.worker_threads),
    ));
    let decode_inflight = Arc::new(AtomicUsize::new(0));
    let scale_inflight = Arc::new(AtomicUsize::new(0));
    let encode_inflight = Arc::new(AtomicUsize::new(0));
    let adaptive_completed = Arc::new(AtomicUsize::new(0));
    let adaptive_failed = Arc::new(AtomicUsize::new(0));
    let adaptive_total_elapsed_ms = Arc::new(AtomicU64::new(0));

    let adaptive_handle = start_adaptive_control_thread(
        config.clone(),
        Arc::clone(&running),
        Arc::clone(&queued_visible),
        Arc::clone(&queued_prefetch),
        Arc::clone(&queued_background),
        Arc::clone(&worker_budget),
        Arc::clone(&adaptive_completed),
        Arc::clone(&adaptive_failed),
        Arc::clone(&adaptive_total_elapsed_ms),
    );
    workers.push(adaptive_handle);

    for i in 0..config.worker_threads {
        let handle = create_worker_thread(
            i,
            config.folder_search_depth,
            config.clone(),
            visible_boost_factor,
            side_boost_factor,
            visible_boost_quota,
            default_quota,
            side_boost_quota,
            Arc::clone(&worker_budget),
            Arc::clone(&decode_inflight),
            Arc::clone(&scale_inflight),
            Arc::clone(&encode_inflight),
            Arc::clone(&adaptive_completed),
            Arc::clone(&adaptive_failed),
            Arc::clone(&adaptive_total_elapsed_ms),
            app.clone(),
            Arc::clone(&task_queue),
            Arc::clone(&current_dir),
            Arc::clone(&request_epoch),
            Arc::clone(&scheduler_paused),
            Arc::clone(&running),
            Arc::clone(&active_workers),
            Arc::clone(&queued_visible),
            Arc::clone(&queued_prefetch),
            Arc::clone(&queued_background),
            Arc::clone(&processed_visible),
            Arc::clone(&processed_prefetch),
            Arc::clone(&processed_background),
            Arc::clone(&decode_wait_count),
            Arc::clone(&decode_wait_ms),
            Arc::clone(&scale_wait_count),
            Arc::clone(&scale_wait_ms),
            Arc::clone(&encode_wait_count),
            Arc::clone(&encode_wait_ms),
            Arc::clone(&memory_cache),
            Arc::clone(&memory_cache_bytes),
            Arc::clone(&db),
            Arc::clone(&generator),
            Arc::clone(&db_index),
            Arc::clone(&folder_db_index),
            Arc::clone(&failed_index),
            Arc::clone(&save_queue),
            Arc::clone(&request_deduplicator),
        );
        workers.push(handle);
    }
    workers
}

/// åˆ›å»ºå•ä¸ªå·¥ä½œçº¿ç¨‹
#[allow(clippy::too_many_arguments)]
fn create_worker_thread(
    worker_id: usize,
    folder_depth: u32,
    config: ThumbnailServiceConfig,
    visible_boost_factor: usize,
    side_boost_factor: usize,
    visible_boost_quota: LaneQuota,
    default_quota: LaneQuota,
    side_boost_quota: LaneQuota,
    worker_budget: Arc<AtomicUsize>,
    decode_inflight: Arc<AtomicUsize>,
    scale_inflight: Arc<AtomicUsize>,
    encode_inflight: Arc<AtomicUsize>,
    adaptive_completed: Arc<AtomicUsize>,
    adaptive_failed: Arc<AtomicUsize>,
    adaptive_total_elapsed_ms: Arc<AtomicU64>,
    app: AppHandle,
    task_queue: Arc<(Mutex<queue::TaskQueueState>, Condvar)>,
    current_dir: Arc<RwLock<String>>,
    request_epoch: Arc<AtomicU64>,
    scheduler_paused: Arc<AtomicBool>,
    running: Arc<AtomicBool>,
    active_workers: Arc<AtomicUsize>,
    queued_visible: Arc<AtomicUsize>,
    queued_prefetch: Arc<AtomicUsize>,
    queued_background: Arc<AtomicUsize>,
    processed_visible: Arc<AtomicUsize>,
    processed_prefetch: Arc<AtomicUsize>,
    processed_background: Arc<AtomicUsize>,
    decode_wait_count: Arc<AtomicUsize>,
    decode_wait_ms: Arc<AtomicU64>,
    scale_wait_count: Arc<AtomicUsize>,
    scale_wait_ms: Arc<AtomicU64>,
    encode_wait_count: Arc<AtomicUsize>,
    encode_wait_ms: Arc<AtomicU64>,
    memory_cache: Arc<RwLock<LruCache<String, Vec<u8>>>>,
    memory_cache_bytes: Arc<AtomicUsize>,
    db: Arc<ThumbnailDb>,
    generator: Arc<ThumbnailGenerator>,
    db_index: Arc<RwLock<HashSet<String>>>,
    folder_db_index: Arc<RwLock<HashSet<String>>>,
    failed_index: Arc<RwLock<HashSet<String>>>,
    save_queue: Arc<Mutex<HashMap<String, (Vec<u8>, i64, i32, Instant)>>>,
    request_deduplicator: Arc<RequestDeduplicator>,
) -> JoinHandle<()> {
    thread::spawn(move || {
        const EMIT_BATCH_SIZE: usize = 16;
        const STAGE_BACKOFF_MS: u64 = 2;
        log_debug!("ğŸ”§ Worker {} started", worker_id);
        let mut emit_batch: Vec<ThumbnailReadyPayload> = Vec::with_capacity(EMIT_BATCH_SIZE);
        let mut lane_tick: usize = worker_id;

        while running.load(Ordering::SeqCst) {
            if scheduler_paused.load(Ordering::Acquire) {
                thread::sleep(Duration::from_millis(20));
                continue;
            }

            // åœ¨å°è¯•å–ä»»åŠ¡ä¹‹å‰ï¼Œè‹¥ emit_batch æœ‰ç§¯å‹ä¸”é˜Ÿåˆ—ä¸ºç©ºï¼Œç«‹å³ flushã€‚
            // è§£å†³å¤š worker åœºæ™¯ä¸‹ï¼šWorker A æœ‰ 1 ä¸ª batch itemï¼ŒWorker B æ‹¿èµ°æœ€åä¸€ä¸ª
            // ä»»åŠ¡å Worker A è¿›å…¥ idle ç­‰å¾…ï¼Œbatch æ°¸è¿œä¸è¢«å‘å°„çš„é—®é¢˜ã€‚
            if !emit_batch.is_empty() {
                let queue_is_empty = backlog_is_empty(
                    &queued_visible,
                    &queued_prefetch,
                    &queued_background,
                );
                if queue_is_empty {
                    flush_worker_emit_batch(&app, &mut emit_batch, true, EMIT_BATCH_SIZE);
                }
            }

            if active_workers.load(Ordering::Acquire) >= worker_budget.load(Ordering::Acquire) {
                thread::sleep(Duration::from_millis(2));
                continue;
            }

            let task = {
                let (queue_lock, queue_cv) = (&task_queue.0, &task_queue.1);
                let mut guard = match queue_lock.lock() {
                    Ok(g) => g,
                    Err(_) => {
                        thread::sleep(Duration::from_millis(10));
                        continue;
                    }
                };

                // è‹¥é˜Ÿåˆ—éç©ºï¼Œç›´æ¥å–ä»»åŠ¡ï¼ˆé¿å…çŸ­æœŸ Condvar ç­‰å¾…ï¼‰
                if !guard.is_empty() {
                    let visible = queued_visible.load(Ordering::Relaxed);
                    let prefetch = queued_prefetch.load(Ordering::Relaxed);
                    let background = queued_background.load(Ordering::Relaxed);
                    let preferred = preferred_lane_for_tick(
                        lane_tick,
                        visible,
                        prefetch,
                        background,
                        visible_boost_factor,
                        side_boost_factor,
                        visible_boost_quota,
                        default_quota,
                        side_boost_quota,
                    );
                    lane_tick = lane_tick.wrapping_add(1);
                    queue::pop_task_by_lane_locked(
                        &mut guard,
                        preferred,
                        &queued_visible,
                        &queued_prefetch,
                        &queued_background,
                    )
                } else if !running.load(Ordering::SeqCst) {
                    None
                } else {
                    // é˜Ÿåˆ—ä¸ºç©ºä¸”ä»åœ¨è¿è¡Œï¼šçŸ­æš‚ç­‰å¾…ï¼Œè¶…æ—¶åé‡Šæ”¾é”è®©å¤–å±‚å¾ªç¯æ£€æŸ¥ emit_batch
                    match queue_cv.wait_timeout(guard, Duration::from_millis(50)) {
                        Ok((mut g, _)) => {
                            if !running.load(Ordering::SeqCst) {
                                None
                            } else {
                                let visible = queued_visible.load(Ordering::Relaxed);
                                let prefetch = queued_prefetch.load(Ordering::Relaxed);
                                let background = queued_background.load(Ordering::Relaxed);
                                let preferred = preferred_lane_for_tick(
                                    lane_tick,
                                    visible,
                                    prefetch,
                                    background,
                                    visible_boost_factor,
                                    side_boost_factor,
                                    visible_boost_quota,
                                    default_quota,
                                    side_boost_quota,
                                );
                                lane_tick = lane_tick.wrapping_add(1);
                                queue::pop_task_by_lane_locked(
                                    &mut g,
                                    preferred,
                                    &queued_visible,
                                    &queued_prefetch,
                                    &queued_background,
                                ) // None if still empty â†’ outer loop flushes
                            }
                        }
                        Err(poisoned) => {
                            let (mut g, _) = poisoned.into_inner();
                            let visible = queued_visible.load(Ordering::Relaxed);
                            let prefetch = queued_prefetch.load(Ordering::Relaxed);
                            let background = queued_background.load(Ordering::Relaxed);
                            let preferred = preferred_lane_for_tick(
                                lane_tick,
                                visible,
                                prefetch,
                                background,
                                visible_boost_factor,
                                side_boost_factor,
                                visible_boost_quota,
                                default_quota,
                                side_boost_quota,
                            );
                            lane_tick = lane_tick.wrapping_add(1);
                            queue::pop_task_by_lane_locked(
                                &mut g,
                                preferred,
                                &queued_visible,
                                &queued_prefetch,
                                &queued_background,
                            )
                        }
                    }
                }
            };

            if let Some(task) = task {
                let should_process = check_task_validity(&task, &current_dir, &request_epoch);
                if !should_process {
                    log_debug!("â­ï¸ è·³è¿‡è¿‡æœŸ/éå½“å‰ç›®å½•ä»»åŠ¡: {}", task.path);
                    request_deduplicator.release_with_id(&task.dedup_key, task.dedup_request_id);
                    continue;
                }

                if needs_decode_limit(&task)
                    && decode_inflight.load(Ordering::Acquire)
                        >= config.decode_stage_max_active.max(1)
                {
                    decode_wait_count.fetch_add(1, Ordering::Relaxed);
                    decode_wait_ms.fetch_add(STAGE_BACKOFF_MS, Ordering::Relaxed);
                    queue::requeue_front(
                        &task_queue,
                        task,
                        &queued_visible,
                        &queued_prefetch,
                        &queued_background,
                    );
                    thread::sleep(Duration::from_millis(STAGE_BACKOFF_MS));
                    continue;
                }

                if needs_scale_limit(&task)
                    && scale_inflight.load(Ordering::Acquire)
                        >= config.scale_stage_max_active.max(1)
                {
                    scale_wait_count.fetch_add(1, Ordering::Relaxed);
                    scale_wait_ms.fetch_add(STAGE_BACKOFF_MS, Ordering::Relaxed);
                    queue::requeue_front(
                        &task_queue,
                        task,
                        &queued_visible,
                        &queued_prefetch,
                        &queued_background,
                    );
                    thread::sleep(Duration::from_millis(STAGE_BACKOFF_MS));
                    continue;
                }

                let mut decode_token_held = false;
                if needs_decode_limit(&task) {
                    decode_inflight.fetch_add(1, Ordering::SeqCst);
                    decode_token_held = true;
                }

                let mut scale_token_held = false;
                if needs_scale_limit(&task) {
                    scale_inflight.fetch_add(1, Ordering::SeqCst);
                    scale_token_held = true;
                }

                active_workers.fetch_add(1, Ordering::SeqCst);
                match task.lane {
                    TaskLane::Visible => {
                        processed_visible.fetch_add(1, Ordering::Relaxed);
                    }
                    TaskLane::Prefetch => {
                        processed_prefetch.fetch_add(1, Ordering::Relaxed);
                    }
                    TaskLane::Background => {
                        processed_background.fetch_add(1, Ordering::Relaxed);
                    }
                }

                let started = Instant::now();
                let mut task_succeeded = false;
                let generated = generate_task_blob(
                    &task,
                    &generator,
                    &db,
                    folder_depth,
                    &failed_index,
                );

                if decode_token_held {
                    decode_inflight.fetch_sub(1, Ordering::SeqCst);
                }
                if scale_token_held {
                    scale_inflight.fetch_sub(1, Ordering::SeqCst);
                }

                if let Some((blob, save_info)) = generated {
                    let mut encode_token_held = false;
                    let encode_wait_started = Instant::now();
                    if needs_encode_limit(&task) {
                        while running.load(Ordering::SeqCst)
                            && encode_inflight.load(Ordering::Acquire)
                                >= config.encode_stage_max_active.max(1)
                        {
                            thread::sleep(Duration::from_millis(1));
                        }
                        if running.load(Ordering::SeqCst) {
                            encode_inflight.fetch_add(1, Ordering::SeqCst);
                            encode_token_held = true;
                        }
                    }
                    let encode_wait_elapsed = encode_wait_started.elapsed().as_millis() as u64;
                    if encode_wait_elapsed > 0 {
                        encode_wait_count.fetch_add(1, Ordering::Relaxed);
                        encode_wait_ms.fetch_add(encode_wait_elapsed, Ordering::Relaxed);
                    }

                    let payload = handle_success(
                        &task,
                        blob,
                        save_info,
                        &memory_cache,
                        &memory_cache_bytes,
                        &db_index,
                        &folder_db_index,
                        &save_queue,
                    );
                    emit_batch.push(payload);
                    let queue_is_empty = backlog_is_empty(
                        &queued_visible,
                        &queued_prefetch,
                        &queued_background,
                    );
                    flush_worker_emit_batch(&app, &mut emit_batch, queue_is_empty, EMIT_BATCH_SIZE);
                    task_succeeded = true;

                    if encode_token_held {
                        encode_inflight.fetch_sub(1, Ordering::SeqCst);
                    }
                }
                active_workers.fetch_sub(1, Ordering::SeqCst);

                request_deduplicator.release_with_id(&task.dedup_key, task.dedup_request_id);

                let elapsed_ms = started.elapsed().as_millis() as u64;
                adaptive_total_elapsed_ms.fetch_add(elapsed_ms, Ordering::Relaxed);
                adaptive_completed.fetch_add(1, Ordering::Relaxed);
                if !task_succeeded {
                    adaptive_failed.fetch_add(1, Ordering::Relaxed);
                }
            } else {
                flush_worker_emit_batch(&app, &mut emit_batch, true, EMIT_BATCH_SIZE);
            }
        }

        flush_worker_emit_batch(&app, &mut emit_batch, true, EMIT_BATCH_SIZE);
        log_debug!("ğŸ”§ Worker {} stopped", worker_id);
    })
}

fn flush_worker_emit_batch(
    app: &AppHandle,
    emit_batch: &mut Vec<ThumbnailReadyPayload>,
    force: bool,
    batch_size: usize,
) {
    if emit_batch.is_empty() {
        return;
    }
    if !force && emit_batch.len() < batch_size {
        return;
    }

    let payload = ThumbnailBatchReadyPayload {
        items: std::mem::take(emit_batch),
    };
    let _ = app.emit("thumbnail-batch-ready", payload);
}

/// æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åº”è¯¥å¤„ç†ï¼ˆç›®å½•æ˜¯å¦åŒ¹é…ï¼‰
fn check_task_validity(
    task: &GenerateTask,
    current_dir: &Arc<RwLock<String>>,
    request_epoch: &Arc<AtomicU64>,
) -> bool {
    if task.request_epoch != request_epoch.load(Ordering::Acquire) {
        return false;
    }
    if task.directory.is_empty() {
        return true;
    }
    // æŒè¯»é”ç›´æ¥æ¯”è¾ƒï¼Œä¸ clone æ•´ä¸ª String
    match current_dir.read() {
        Ok(guard) => task.directory == *guard,
        Err(_) => false,
    }
}

/// å¤„ç†å•ä¸ªä»»åŠ¡
#[allow(clippy::too_many_arguments)]
fn generate_task_blob(
    task: &GenerateTask,
    generator: &Arc<ThumbnailGenerator>,
    db: &Arc<ThumbnailDb>,
    folder_depth: u32,
    failed_index: &Arc<RwLock<HashSet<String>>>,
) -> Option<(Vec<u8>, Option<(String, i64, i32)>)> {

    let gen_result = panic::catch_unwind(panic::AssertUnwindSafe(|| match task.file_type {
        ThumbnailFileType::Folder => {
            generate_folder_thumbnail_static(generator, db, &task.path, folder_depth)
                .map(|blob| (blob, None))
        }
        ThumbnailFileType::Archive => generate_archive_thumbnail_static(generator, &task.path)
            .map(|(blob, pk, sz, gh)| (blob, Some((pk, sz, gh)))),
        ThumbnailFileType::Video => generate_video_thumbnail_static(generator, &task.path)
            .map(|(blob, pk, sz, gh)| (blob, Some((pk, sz, gh)))),
        ThumbnailFileType::Image | ThumbnailFileType::Other => {
            generate_file_thumbnail_static(generator, &task.path)
                .map(|(blob, pk, sz, gh)| (blob, Some((pk, sz, gh))))
        }
    }));

    match gen_result {
        Ok(Ok((blob, save_info))) => {
            return Some((blob, save_info));
        }
        Ok(Err(e)) => {
            log_debug!("âš ï¸ ç”Ÿæˆç¼©ç•¥å›¾å¤±è´¥: {} - {}", task.path, e);
            // æ–‡ä»¶å¤¹å¤±è´¥ä¸åŠ å…¥ failed_indexï¼šå­æ–‡ä»¶å¯èƒ½å°šæœªç”Ÿæˆç¼©ç•¥å›¾ï¼Œéœ€è¦å…è®¸é‡è¯•
            if !matches!(task.file_type, ThumbnailFileType::Folder) {
                if let Ok(mut idx) = failed_index.write() {
                    idx.insert(task.path.clone());
                }
            }
        }
        Err(_) => {
            log_debug!("âš ï¸ ç”Ÿæˆç¼©ç•¥å›¾æ—¶ panic: {}", task.path);
            // æ–‡ä»¶å¤¹ panic ä¹Ÿä¸åŠ å…¥æ°¸ä¹…å¤±è´¥åˆ—è¡¨ï¼Œå…è®¸åç»­é‡è¯•
            if !matches!(task.file_type, ThumbnailFileType::Folder) {
                if let Ok(mut idx) = failed_index.write() {
                    idx.insert(task.path.clone());
                }
            }
        }
    }

    None
}

/// å¤„ç†æˆåŠŸç”Ÿæˆçš„ç¼©ç•¥å›¾ï¼ˆæ¥æ”¶ owned blob é¿å…å¤šä½™ to_vecï¼‰
#[allow(clippy::too_many_arguments)]
fn handle_success(
    task: &GenerateTask,
    blob: Vec<u8>,
    save_info: Option<(String, i64, i32)>,
    memory_cache: &Arc<RwLock<LruCache<String, Vec<u8>>>>,
    memory_cache_bytes: &Arc<AtomicUsize>,
    db_index: &Arc<RwLock<HashSet<String>>>,
    folder_db_index: &Arc<RwLock<HashSet<String>>>,
    save_queue: &Arc<Mutex<HashMap<String, (Vec<u8>, i64, i32, Instant)>>>,
) -> ThumbnailReadyPayload {
    let blob_len = blob.len();
    // æ”¾å…¥ä¿å­˜é˜Ÿåˆ—ï¼ˆå¦‚æœ‰éœ€è¦ï¼Œå…ˆ clone å† move blob åˆ°å†…å­˜ç¼“å­˜ï¼Œçœä¸€æ¬¡ to_vecï¼‰
    if let Some((path_key, size, ghash)) = save_info {
        if let Ok(mut q) = save_queue.lock() {
            q.insert(path_key, (blob.clone(), size, ghash, Instant::now()));
        }
    }
    // æ›´æ–°å†…å­˜ç¼“å­˜ï¼ˆmove blobï¼Œé›¶æ‹·è´ï¼‰
    if let Ok(mut cache) = memory_cache.write() {
        cache.put(task.path.clone(), blob);
        memory_cache_bytes.fetch_add(blob_len, Ordering::SeqCst);
    }
    // æ›´æ–°æ•°æ®åº“ç´¢å¼•
    if let Ok(mut idx) = db_index.write() {
        idx.insert(task.path.clone());
    }
    // å¦‚æœæ˜¯æ–‡ä»¶å¤¹ï¼Œæ›´æ–°æ–‡ä»¶å¤¹ç´¢å¼•
    if matches!(task.file_type, ThumbnailFileType::Folder) {
        if let Ok(mut idx) = folder_db_index.write() {
            idx.insert(task.path.clone());
        }
    }
    // IPC ä¸å†ä¼ è¾“ blobï¼šå‰ç«¯é€šè¿‡åè®® URL /thumb/{key} ç›´æ¥ä»å†…å­˜ç¼“å­˜è¯»å–
    ThumbnailReadyPayload {
        path: task.path.clone(),
    }
}

/// å¯åŠ¨ä¿å­˜é˜Ÿåˆ—åˆ·æ–°çº¿ç¨‹
pub fn start_flush_thread(
    running: Arc<AtomicBool>,
    save_queue: Arc<Mutex<HashMap<String, (Vec<u8>, i64, i32, Instant)>>>,
    db: Arc<ThumbnailDb>,
    flush_interval_ms: u64,
    batch_threshold: usize,
) -> JoinHandle<()> {
    thread::spawn(move || {
        log_debug!(
            "ğŸ”§ SaveQueue flush thread started (batch_threshold={})",
            batch_threshold
        );
        let mut last_flush = Instant::now();
        while running.load(Ordering::SeqCst) {
            thread::sleep(Duration::from_millis(500));
            let (should_flush, _) =
                check_flush_condition(&save_queue, &last_flush, flush_interval_ms, batch_threshold);
            if !should_flush {
                continue;
            }
            let items = drain_save_queue(&save_queue);
            if items.is_empty() {
                continue;
            }
            last_flush = Instant::now();
            log_debug!("ğŸ’¾ æ‰¹é‡ä¿å­˜ {} ä¸ªç¼©ç•¥å›¾åˆ°æ•°æ®åº“", items.len());
            save_items_to_db(&db, items);
        }
        // é€€å‡ºå‰åˆ·æ–°å‰©ä½™é˜Ÿåˆ—
        let remaining = drain_save_queue(&save_queue);
        if !remaining.is_empty() {
            log_debug!("ğŸ’¾ é€€å‡ºå‰æ‰¹é‡ä¿å­˜ {} ä¸ªç¼©ç•¥å›¾", remaining.len());
            save_items_to_db(&db, remaining);
        }
        log_debug!("ğŸ”§ SaveQueue flush thread stopped");
    })
}

/// æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ·æ–°ä¿å­˜é˜Ÿåˆ—
fn check_flush_condition(
    save_queue: &Arc<Mutex<HashMap<String, (Vec<u8>, i64, i32, Instant)>>>,
    last_flush: &Instant,
    flush_interval_ms: u64,
    batch_threshold: usize,
) -> (bool, usize) {
    match save_queue.lock() {
        Ok(q) => {
            let len = q.len();
            let time_ok = last_flush.elapsed().as_millis() as u64 >= flush_interval_ms;
            let count_ok = len >= batch_threshold;
            ((time_ok || count_ok) && len > 0, len)
        }
        Err(_) => (false, 0),
    }
}

/// æ¸…ç©ºä¿å­˜é˜Ÿåˆ—å¹¶è¿”å›æ‰€æœ‰é¡¹
fn drain_save_queue(
    save_queue: &Arc<Mutex<HashMap<String, (Vec<u8>, i64, i32, Instant)>>>,
) -> Vec<(String, i64, i32, Vec<u8>)> {
    match save_queue.lock() {
        Ok(mut q) => q.drain().map(|(k, (b, s, g, _))| (k, s, g, b)).collect(),
        Err(_) => Vec::new(),
    }
}

/// ä¿å­˜é¡¹åˆ°æ•°æ®åº“
fn save_items_to_db(db: &Arc<ThumbnailDb>, items: Vec<(String, i64, i32, Vec<u8>)>) {
    if let Err(e) = db.save_thumbnails_batch(&items) {
        log_debug!("âš ï¸ æ‰¹é‡ä¿å­˜å¤±è´¥: {}, å›é€€åˆ°é€ä¸ªä¿å­˜", e);
        for (pk, sz, gh, blob) in items {
            let _ = db.save_thumbnail(&pk, sz, gh, &blob);
        }
    }
}
