//! å·¥ä½œçº¿ç¨‹æ¨¡å—
//! åŒ…å«å·¥ä½œçº¿ç¨‹å¯åŠ¨é€»è¾‘ã€ä»»åŠ¡å¤„ç†å¾ªç¯ã€ä¿å­˜é˜Ÿåˆ—åˆ·æ–°çº¿ç¨‹

use lru::LruCache;
use std::collections::{HashMap, HashSet};
use std::panic;
use std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};
use std::sync::{Arc, Condvar, Mutex, RwLock};
use std::thread::{self, JoinHandle};
use std::time::{Duration, Instant};
use tauri::{AppHandle, Emitter};

use crate::core::thumbnail_db::ThumbnailDb;
use crate::core::thumbnail_generator::ThumbnailGenerator;
use crate::core::request_dedup::RequestDeduplicator;

use super::config::ThumbnailServiceConfig;
use super::generators::{
    generate_archive_thumbnail_static, generate_file_thumbnail_static,
    generate_folder_thumbnail_static, generate_video_thumbnail_static,
};
use super::queue;
use super::types::{
    GenerateTask, TaskLane, ThumbnailBatchReadyPayload, ThumbnailFileType, ThumbnailReadyPayload,
};
use super::{log_debug, log_info};

const LANE_SCHEDULE_LEN: usize = 9;

fn preferred_lane_for_tick(tick: usize, visible: usize, prefetch: usize, background: usize) -> TaskLane {
    let side_total = prefetch + background;

    // visible ç§¯å‹æ˜æ˜¾æ—¶ï¼Œæå‡å‰å°é…é¢ï¼ˆ8:1:1ï¼‰
    if visible > 0 && visible >= side_total {
        return match tick % 10 {
            0 | 1 | 2 | 3 | 4 | 5 | 6 | 8 => TaskLane::Visible,
            7 => TaskLane::Prefetch,
            _ => TaskLane::Background,
        };
    }

    // åå°/é¢„å–ç§¯å‹æ˜æ˜¾æ—¶ï¼Œæ”¾å®½ä¸º 4:3:3 æå‡æ€»ä½“åå
    if side_total > visible.saturating_mul(2) {
        return match tick % 10 {
            0 | 3 | 6 | 9 => TaskLane::Visible,
            1 | 4 | 7 => TaskLane::Prefetch,
            _ => TaskLane::Background,
        };
    }

    // é»˜è®¤ 6:2:1 æ—¶é—´ç‰‡é…é¢ï¼ˆvisible:prefetch:backgroundï¼‰
    match tick % LANE_SCHEDULE_LEN {
        0 | 1 | 2 | 4 | 5 | 7 => TaskLane::Visible,
        3 | 6 => TaskLane::Prefetch,
        _ => TaskLane::Background,
    }
}

/// å¯åŠ¨å·¥ä½œçº¿ç¨‹
#[allow(clippy::too_many_arguments)]
pub fn start_workers(
    config: &ThumbnailServiceConfig,
    running: Arc<AtomicBool>,
    task_queue: Arc<(Mutex<queue::TaskQueueState>, Condvar)>,
    current_dir: Arc<RwLock<String>>,
    request_epoch: Arc<AtomicU64>,
    scheduler_paused: Arc<AtomicBool>,
    active_workers: Arc<AtomicUsize>,
    queued_visible: Arc<AtomicUsize>,
    queued_prefetch: Arc<AtomicUsize>,
    queued_background: Arc<AtomicUsize>,
    processed_visible: Arc<AtomicUsize>,
    processed_prefetch: Arc<AtomicUsize>,
    processed_background: Arc<AtomicUsize>,
    memory_cache: Arc<RwLock<LruCache<String, Vec<u8>>>>,
    memory_cache_bytes: Arc<AtomicUsize>,
    db: Arc<ThumbnailDb>,
    generator: Arc<ThumbnailGenerator>,
    db_index: Arc<RwLock<HashSet<String>>>,
    folder_db_index: Arc<RwLock<HashSet<String>>>,
    failed_index: Arc<RwLock<HashSet<String>>>,
    save_queue: Arc<Mutex<HashMap<String, (Vec<u8>, i64, i32, Instant)>>>,
    request_deduplicator: Arc<RequestDeduplicator>,
    app: AppHandle,
) -> Vec<JoinHandle<()>> {
    let mut workers = Vec::new();
    for i in 0..config.worker_threads {
        let handle = create_worker_thread(
            i,
            config.folder_search_depth,
            app.clone(),
            Arc::clone(&task_queue),
            Arc::clone(&current_dir),
            Arc::clone(&request_epoch),
            Arc::clone(&scheduler_paused),
            Arc::clone(&running),
            Arc::clone(&active_workers),
            Arc::clone(&queued_visible),
            Arc::clone(&queued_prefetch),
            Arc::clone(&queued_background),
            Arc::clone(&processed_visible),
            Arc::clone(&processed_prefetch),
            Arc::clone(&processed_background),
            Arc::clone(&memory_cache),
            Arc::clone(&memory_cache_bytes),
            Arc::clone(&db),
            Arc::clone(&generator),
            Arc::clone(&db_index),
            Arc::clone(&folder_db_index),
            Arc::clone(&failed_index),
            Arc::clone(&save_queue),
            Arc::clone(&request_deduplicator),
        );
        workers.push(handle);
    }
    workers
}

/// åˆ›å»ºå•ä¸ªå·¥ä½œçº¿ç¨‹
#[allow(clippy::too_many_arguments)]
fn create_worker_thread(
    worker_id: usize,
    folder_depth: u32,
    app: AppHandle,
    task_queue: Arc<(Mutex<queue::TaskQueueState>, Condvar)>,
    current_dir: Arc<RwLock<String>>,
    request_epoch: Arc<AtomicU64>,
    scheduler_paused: Arc<AtomicBool>,
    running: Arc<AtomicBool>,
    active_workers: Arc<AtomicUsize>,
    queued_visible: Arc<AtomicUsize>,
    queued_prefetch: Arc<AtomicUsize>,
    queued_background: Arc<AtomicUsize>,
    processed_visible: Arc<AtomicUsize>,
    processed_prefetch: Arc<AtomicUsize>,
    processed_background: Arc<AtomicUsize>,
    memory_cache: Arc<RwLock<LruCache<String, Vec<u8>>>>,
    memory_cache_bytes: Arc<AtomicUsize>,
    db: Arc<ThumbnailDb>,
    generator: Arc<ThumbnailGenerator>,
    db_index: Arc<RwLock<HashSet<String>>>,
    folder_db_index: Arc<RwLock<HashSet<String>>>,
    failed_index: Arc<RwLock<HashSet<String>>>,
    save_queue: Arc<Mutex<HashMap<String, (Vec<u8>, i64, i32, Instant)>>>,
    request_deduplicator: Arc<RequestDeduplicator>,
) -> JoinHandle<()> {
    thread::spawn(move || {
        const EMIT_BATCH_SIZE: usize = 16;
        log_debug!("ğŸ”§ Worker {} started", worker_id);
        let mut emit_batch: Vec<ThumbnailReadyPayload> = Vec::with_capacity(EMIT_BATCH_SIZE);
        let mut lane_tick: usize = worker_id % LANE_SCHEDULE_LEN;

        while running.load(Ordering::SeqCst) {
            if scheduler_paused.load(Ordering::Acquire) {
                thread::sleep(Duration::from_millis(20));
                continue;
            }

            // åœ¨å°è¯•å–ä»»åŠ¡ä¹‹å‰ï¼Œè‹¥ emit_batch æœ‰ç§¯å‹ä¸”é˜Ÿåˆ—ä¸ºç©ºï¼Œç«‹å³ flushã€‚
            // è§£å†³å¤š worker åœºæ™¯ä¸‹ï¼šWorker A æœ‰ 1 ä¸ª batch itemï¼ŒWorker B æ‹¿èµ°æœ€åä¸€ä¸ª
            // ä»»åŠ¡å Worker A è¿›å…¥ idle ç­‰å¾…ï¼Œbatch æ°¸è¿œä¸è¢«å‘å°„çš„é—®é¢˜ã€‚
            if !emit_batch.is_empty() {
                let queue_is_empty = task_queue.0.lock().map(|q| q.is_empty()).unwrap_or(false);
                if queue_is_empty {
                    flush_worker_emit_batch(&app, &mut emit_batch, true, EMIT_BATCH_SIZE);
                }
            }

            let task = {
                let (queue_lock, queue_cv) = (&task_queue.0, &task_queue.1);
                let mut guard = match queue_lock.lock() {
                    Ok(g) => g,
                    Err(_) => {
                        thread::sleep(Duration::from_millis(10));
                        continue;
                    }
                };

                // è‹¥é˜Ÿåˆ—éç©ºï¼Œç›´æ¥å–ä»»åŠ¡ï¼ˆé¿å…çŸ­æœŸ Condvar ç­‰å¾…ï¼‰
                if !guard.is_empty() {
                    let visible = queued_visible.load(Ordering::Relaxed);
                    let prefetch = queued_prefetch.load(Ordering::Relaxed);
                    let background = queued_background.load(Ordering::Relaxed);
                    let preferred = preferred_lane_for_tick(lane_tick, visible, prefetch, background);
                    lane_tick = lane_tick.wrapping_add(1);
                    queue::pop_task_by_lane_locked(
                        &mut guard,
                        preferred,
                        &queued_visible,
                        &queued_prefetch,
                        &queued_background,
                    )
                } else if !running.load(Ordering::SeqCst) {
                    None
                } else {
                    // é˜Ÿåˆ—ä¸ºç©ºä¸”ä»åœ¨è¿è¡Œï¼šçŸ­æš‚ç­‰å¾…ï¼Œè¶…æ—¶åé‡Šæ”¾é”è®©å¤–å±‚å¾ªç¯æ£€æŸ¥ emit_batch
                    match queue_cv.wait_timeout(guard, Duration::from_millis(50)) {
                        Ok((mut g, _)) => {
                            if !running.load(Ordering::SeqCst) {
                                None
                            } else {
                                let visible = queued_visible.load(Ordering::Relaxed);
                                let prefetch = queued_prefetch.load(Ordering::Relaxed);
                                let background = queued_background.load(Ordering::Relaxed);
                                let preferred =
                                    preferred_lane_for_tick(lane_tick, visible, prefetch, background);
                                lane_tick = lane_tick.wrapping_add(1);
                                queue::pop_task_by_lane_locked(
                                    &mut g,
                                    preferred,
                                    &queued_visible,
                                    &queued_prefetch,
                                    &queued_background,
                                ) // None if still empty â†’ outer loop flushes
                            }
                        }
                        Err(poisoned) => {
                            let (mut g, _) = poisoned.into_inner();
                            let visible = queued_visible.load(Ordering::Relaxed);
                            let prefetch = queued_prefetch.load(Ordering::Relaxed);
                            let background = queued_background.load(Ordering::Relaxed);
                            let preferred =
                                preferred_lane_for_tick(lane_tick, visible, prefetch, background);
                            lane_tick = lane_tick.wrapping_add(1);
                            queue::pop_task_by_lane_locked(
                                &mut g,
                                preferred,
                                &queued_visible,
                                &queued_prefetch,
                                &queued_background,
                            )
                        }
                    }
                }
            };

            if let Some(task) = task {
                let should_process = check_task_validity(&task, &current_dir, &request_epoch);
                if !should_process {
                    log_debug!("â­ï¸ è·³è¿‡è¿‡æœŸ/éå½“å‰ç›®å½•ä»»åŠ¡: {}", task.path);
                    request_deduplicator.release_with_id(&task.dedup_key, task.dedup_request_id);
                    continue;
                }
                active_workers.fetch_add(1, Ordering::SeqCst);
                match task.lane {
                    TaskLane::Visible => {
                        processed_visible.fetch_add(1, Ordering::Relaxed);
                    }
                    TaskLane::Prefetch => {
                        processed_prefetch.fetch_add(1, Ordering::Relaxed);
                    }
                    TaskLane::Background => {
                        processed_background.fetch_add(1, Ordering::Relaxed);
                    }
                }
                if let Some(payload) = process_task(
                    &task,
                    &generator,
                    &db,
                    folder_depth,
                    &memory_cache,
                    &memory_cache_bytes,
                    &db_index,
                    &folder_db_index,
                    &failed_index,
                    &save_queue,
                    &request_deduplicator,
                ) {
                    emit_batch.push(payload);
                    // ä»»åŠ¡å¤„ç†å®Œåæ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦å·²ç©ºï¼š
                    // è‹¥ç©ºåˆ™ç«‹å³å¼ºåˆ¶å‘å°„ï¼Œé¿å…å°æ‰¹é‡ï¼ˆ< EMIT_BATCH_SIZEï¼‰æ°¸è¿œæ»ç•™
                    // ï¼ˆå…¸å‹åœºæ™¯ï¼šæ–‡ä»¶å¤¹åªæœ‰å‡ ä¸ªå‹ç¼©åŒ…ï¼Œå¤„ç†å®Œåé˜Ÿåˆ—å˜ç©ºä½† batch ä¸æ»¡ 16ï¼‰
                    let queue_is_empty = task_queue.0.lock().map(|q| q.is_empty()).unwrap_or(false);
                    flush_worker_emit_batch(&app, &mut emit_batch, queue_is_empty, EMIT_BATCH_SIZE);
                }
                active_workers.fetch_sub(1, Ordering::SeqCst);
            } else {
                flush_worker_emit_batch(&app, &mut emit_batch, true, EMIT_BATCH_SIZE);
            }
        }

        flush_worker_emit_batch(&app, &mut emit_batch, true, EMIT_BATCH_SIZE);
        log_debug!("ğŸ”§ Worker {} stopped", worker_id);
    })
}

fn flush_worker_emit_batch(
    app: &AppHandle,
    emit_batch: &mut Vec<ThumbnailReadyPayload>,
    force: bool,
    batch_size: usize,
) {
    if emit_batch.is_empty() {
        return;
    }
    if !force && emit_batch.len() < batch_size {
        return;
    }

    let payload = ThumbnailBatchReadyPayload {
        items: std::mem::take(emit_batch),
    };
    let _ = app.emit("thumbnail-batch-ready", payload);
}

/// æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åº”è¯¥å¤„ç†ï¼ˆç›®å½•æ˜¯å¦åŒ¹é…ï¼‰
fn check_task_validity(
    task: &GenerateTask,
    current_dir: &Arc<RwLock<String>>,
    request_epoch: &Arc<AtomicU64>,
) -> bool {
    if task.request_epoch != request_epoch.load(Ordering::Acquire) {
        return false;
    }
    if task.directory.is_empty() {
        return true;
    }
    // æŒè¯»é”ç›´æ¥æ¯”è¾ƒï¼Œä¸ clone æ•´ä¸ª String
    match current_dir.read() {
        Ok(guard) => task.directory == *guard,
        Err(_) => false,
    }
}

/// å¤„ç†å•ä¸ªä»»åŠ¡
#[allow(clippy::too_many_arguments)]
fn process_task(
    task: &GenerateTask,
    generator: &Arc<ThumbnailGenerator>,
    db: &Arc<ThumbnailDb>,
    folder_depth: u32,
    memory_cache: &Arc<RwLock<LruCache<String, Vec<u8>>>>,
    memory_cache_bytes: &Arc<AtomicUsize>,
    db_index: &Arc<RwLock<HashSet<String>>>,
    folder_db_index: &Arc<RwLock<HashSet<String>>>,
    failed_index: &Arc<RwLock<HashSet<String>>>,
    save_queue: &Arc<Mutex<HashMap<String, (Vec<u8>, i64, i32, Instant)>>>,
    request_deduplicator: &Arc<RequestDeduplicator>,
) -> Option<ThumbnailReadyPayload> {
    let mut ready_payload: Option<ThumbnailReadyPayload> = None;

    let gen_result = panic::catch_unwind(panic::AssertUnwindSafe(|| match task.file_type {
        ThumbnailFileType::Folder => {
            generate_folder_thumbnail_static(generator, db, &task.path, folder_depth)
                .map(|blob| (blob, None))
        }
        ThumbnailFileType::Archive => generate_archive_thumbnail_static(generator, &task.path)
            .map(|(blob, pk, sz, gh)| (blob, Some((pk, sz, gh)))),
        ThumbnailFileType::Video => generate_video_thumbnail_static(generator, &task.path)
            .map(|(blob, pk, sz, gh)| (blob, Some((pk, sz, gh)))),
        ThumbnailFileType::Image | ThumbnailFileType::Other => {
            generate_file_thumbnail_static(generator, &task.path)
                .map(|(blob, pk, sz, gh)| (blob, Some((pk, sz, gh))))
        }
    }));

    match gen_result {
        Ok(Ok((blob, save_info))) => {
            ready_payload = Some(handle_success(
                task,
                blob,
                save_info,
                memory_cache,
                memory_cache_bytes,
                db_index,
                folder_db_index,
                save_queue,
            ));
        }
        Ok(Err(e)) => {
            log_debug!("âš ï¸ ç”Ÿæˆç¼©ç•¥å›¾å¤±è´¥: {} - {}", task.path, e);
            // æ–‡ä»¶å¤¹å¤±è´¥ä¸åŠ å…¥ failed_indexï¼šå­æ–‡ä»¶å¯èƒ½å°šæœªç”Ÿæˆç¼©ç•¥å›¾ï¼Œéœ€è¦å…è®¸é‡è¯•
            if !matches!(task.file_type, ThumbnailFileType::Folder) {
                if let Ok(mut idx) = failed_index.write() {
                    idx.insert(task.path.clone());
                }
            }
        }
        Err(_) => {
            log_debug!("âš ï¸ ç”Ÿæˆç¼©ç•¥å›¾æ—¶ panic: {}", task.path);
            // æ–‡ä»¶å¤¹ panic ä¹Ÿä¸åŠ å…¥æ°¸ä¹…å¤±è´¥åˆ—è¡¨ï¼Œå…è®¸åç»­é‡è¯•
            if !matches!(task.file_type, ThumbnailFileType::Folder) {
                if let Ok(mut idx) = failed_index.write() {
                    idx.insert(task.path.clone());
                }
            }
        }
    }

    request_deduplicator.release_with_id(&task.dedup_key, task.dedup_request_id);

    ready_payload
}

/// å¤„ç†æˆåŠŸç”Ÿæˆçš„ç¼©ç•¥å›¾ï¼ˆæ¥æ”¶ owned blob é¿å…å¤šä½™ to_vecï¼‰
#[allow(clippy::too_many_arguments)]
fn handle_success(
    task: &GenerateTask,
    blob: Vec<u8>,
    save_info: Option<(String, i64, i32)>,
    memory_cache: &Arc<RwLock<LruCache<String, Vec<u8>>>>,
    memory_cache_bytes: &Arc<AtomicUsize>,
    db_index: &Arc<RwLock<HashSet<String>>>,
    folder_db_index: &Arc<RwLock<HashSet<String>>>,
    save_queue: &Arc<Mutex<HashMap<String, (Vec<u8>, i64, i32, Instant)>>>,
) -> ThumbnailReadyPayload {
    let blob_len = blob.len();
    // æ”¾å…¥ä¿å­˜é˜Ÿåˆ—ï¼ˆå¦‚æœ‰éœ€è¦ï¼Œå…ˆ clone å† move blob åˆ°å†…å­˜ç¼“å­˜ï¼Œçœä¸€æ¬¡ to_vecï¼‰
    if let Some((path_key, size, ghash)) = save_info {
        if let Ok(mut q) = save_queue.lock() {
            q.insert(path_key, (blob.clone(), size, ghash, Instant::now()));
        }
    }
    // æ›´æ–°å†…å­˜ç¼“å­˜ï¼ˆmove blobï¼Œé›¶æ‹·è´ï¼‰
    if let Ok(mut cache) = memory_cache.write() {
        cache.put(task.path.clone(), blob);
        memory_cache_bytes.fetch_add(blob_len, Ordering::SeqCst);
    }
    // æ›´æ–°æ•°æ®åº“ç´¢å¼•
    if let Ok(mut idx) = db_index.write() {
        idx.insert(task.path.clone());
    }
    // å¦‚æœæ˜¯æ–‡ä»¶å¤¹ï¼Œæ›´æ–°æ–‡ä»¶å¤¹ç´¢å¼•
    if matches!(task.file_type, ThumbnailFileType::Folder) {
        if let Ok(mut idx) = folder_db_index.write() {
            idx.insert(task.path.clone());
        }
    }
    // IPC ä¸å†ä¼ è¾“ blobï¼šå‰ç«¯é€šè¿‡åè®® URL /thumb/{key} ç›´æ¥ä»å†…å­˜ç¼“å­˜è¯»å–
    ThumbnailReadyPayload {
        path: task.path.clone(),
    }
}

/// å¯åŠ¨ä¿å­˜é˜Ÿåˆ—åˆ·æ–°çº¿ç¨‹
pub fn start_flush_thread(
    running: Arc<AtomicBool>,
    save_queue: Arc<Mutex<HashMap<String, (Vec<u8>, i64, i32, Instant)>>>,
    db: Arc<ThumbnailDb>,
    flush_interval_ms: u64,
    batch_threshold: usize,
) -> JoinHandle<()> {
    thread::spawn(move || {
        log_debug!(
            "ğŸ”§ SaveQueue flush thread started (batch_threshold={})",
            batch_threshold
        );
        let mut last_flush = Instant::now();
        while running.load(Ordering::SeqCst) {
            thread::sleep(Duration::from_millis(500));
            let (should_flush, _) =
                check_flush_condition(&save_queue, &last_flush, flush_interval_ms, batch_threshold);
            if !should_flush {
                continue;
            }
            let items = drain_save_queue(&save_queue);
            if items.is_empty() {
                continue;
            }
            last_flush = Instant::now();
            log_debug!("ğŸ’¾ æ‰¹é‡ä¿å­˜ {} ä¸ªç¼©ç•¥å›¾åˆ°æ•°æ®åº“", items.len());
            save_items_to_db(&db, items);
        }
        // é€€å‡ºå‰åˆ·æ–°å‰©ä½™é˜Ÿåˆ—
        let remaining = drain_save_queue(&save_queue);
        if !remaining.is_empty() {
            log_debug!("ğŸ’¾ é€€å‡ºå‰æ‰¹é‡ä¿å­˜ {} ä¸ªç¼©ç•¥å›¾", remaining.len());
            save_items_to_db(&db, remaining);
        }
        log_debug!("ğŸ”§ SaveQueue flush thread stopped");
    })
}

/// æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ·æ–°ä¿å­˜é˜Ÿåˆ—
fn check_flush_condition(
    save_queue: &Arc<Mutex<HashMap<String, (Vec<u8>, i64, i32, Instant)>>>,
    last_flush: &Instant,
    flush_interval_ms: u64,
    batch_threshold: usize,
) -> (bool, usize) {
    match save_queue.lock() {
        Ok(q) => {
            let len = q.len();
            let time_ok = last_flush.elapsed().as_millis() as u64 >= flush_interval_ms;
            let count_ok = len >= batch_threshold;
            ((time_ok || count_ok) && len > 0, len)
        }
        Err(_) => (false, 0),
    }
}

/// æ¸…ç©ºä¿å­˜é˜Ÿåˆ—å¹¶è¿”å›æ‰€æœ‰é¡¹
fn drain_save_queue(
    save_queue: &Arc<Mutex<HashMap<String, (Vec<u8>, i64, i32, Instant)>>>,
) -> Vec<(String, i64, i32, Vec<u8>)> {
    match save_queue.lock() {
        Ok(mut q) => q.drain().map(|(k, (b, s, g, _))| (k, s, g, b)).collect(),
        Err(_) => Vec::new(),
    }
}

/// ä¿å­˜é¡¹åˆ°æ•°æ®åº“
fn save_items_to_db(db: &Arc<ThumbnailDb>, items: Vec<(String, i64, i32, Vec<u8>)>) {
    if let Err(e) = db.save_thumbnails_batch(&items) {
        log_debug!("âš ï¸ æ‰¹é‡ä¿å­˜å¤±è´¥: {}, å›é€€åˆ°é€ä¸ªä¿å­˜", e);
        for (pk, sz, gh, blob) in items {
            let _ = db.save_thumbnail(&pk, sz, gh, &blob);
        }
    }
}
