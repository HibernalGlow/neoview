//! è¶…åˆ†æœåŠ¡å·¥ä½œçº¿ç¨‹æ¨¡å—
//!
//! åŒ…å«å·¥ä½œçº¿ç¨‹å¯åŠ¨é€»è¾‘ã€ä»»åŠ¡å¤„ç†å¾ªç¯

use std::collections::{HashMap, HashSet, VecDeque};
use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering};
use std::sync::{Arc, Mutex, RwLock};
use std::thread::{self, JoinHandle};
use std::time::Duration;
use tauri::{AppHandle, Emitter};

use crate::commands::pyo3_upscale_commands::PyO3UpscalerState;
use crate::commands::upscale_service_commands::FrontendCondition;
use crate::core::upscale_settings::ConditionalUpscaleSettings;

use super::config::UpscaleServiceConfig;
use super::events::{UpscaleReadyPayload, UpscaleStatus};
use super::log_debug;
use super::queue::get_highest_priority_task;
use super::task_processor::process_task_v2;
use super::types::{CacheEntry, TaskPriority, UpscaleTask};

/// å¯åŠ¨å·¥ä½œçº¿ç¨‹
#[allow(clippy::too_many_arguments)]
pub fn start_workers(
    config: &UpscaleServiceConfig,
    app: AppHandle,
    running: Arc<AtomicBool>,
    enabled: Arc<AtomicBool>,
    task_queue: Arc<Mutex<VecDeque<UpscaleTask>>>,
    current_book: Arc<RwLock<Option<String>>>,
    cache_map: Arc<RwLock<HashMap<(String, usize), CacheEntry>>>,
    cache_dir: std::path::PathBuf,
    processing_set: Arc<RwLock<HashSet<(String, usize)>>>,
    skipped_pages: Arc<RwLock<HashSet<(String, usize)>>>,
    failed_pages: Arc<RwLock<HashSet<(String, usize)>>>,
    completed_count: Arc<AtomicUsize>,
    skipped_count: Arc<AtomicUsize>,
    failed_count: Arc<AtomicUsize>,
    py_state: Arc<PyO3UpscalerState>,
    condition_settings: Arc<RwLock<ConditionalUpscaleSettings>>,
    conditions_list: Arc<RwLock<Vec<FrontendCondition>>>,
) -> Vec<JoinHandle<()>> {
    let mut workers = Vec::new();

    for i in 0..config.worker_threads {
        let app = app.clone();
        let running = Arc::clone(&running);
        let enabled = Arc::clone(&enabled);
        let task_queue = Arc::clone(&task_queue);
        let current_book = Arc::clone(&current_book);
        let cache_map = Arc::clone(&cache_map);
        let cache_dir = cache_dir.clone();
        let processing_set = Arc::clone(&processing_set);
        let skipped_pages = Arc::clone(&skipped_pages);
        let failed_pages = Arc::clone(&failed_pages);
        let completed_count = Arc::clone(&completed_count);
        let skipped_count = Arc::clone(&skipped_count);
        let failed_count = Arc::clone(&failed_count);
        let py_state = Arc::clone(&py_state);
        let condition_settings = Arc::clone(&condition_settings);
        let conditions_list = Arc::clone(&conditions_list);
        let default_timeout = config.default_timeout;

        let handle = thread::spawn(move || {
            log_debug!("ğŸ”§ Worker {} started", i);
            worker_loop(
                i,
                running,
                enabled,
                task_queue,
                current_book,
                cache_map,
                cache_dir,
                processing_set,
                skipped_pages,
                failed_pages,
                completed_count,
                skipped_count,
                failed_count,
                py_state,
                condition_settings,
                conditions_list,
                default_timeout,
                app,
            );
            log_debug!("ğŸ”§ Worker {} stopped", i);
        });

        workers.push(handle);
    }

    workers
}

/// å·¥ä½œçº¿ç¨‹ä¸»å¾ªç¯
#[allow(clippy::too_many_arguments)]
fn worker_loop(
    worker_id: usize,
    running: Arc<AtomicBool>,
    enabled: Arc<AtomicBool>,
    task_queue: Arc<Mutex<VecDeque<UpscaleTask>>>,
    current_book: Arc<RwLock<Option<String>>>,
    cache_map: Arc<RwLock<HashMap<(String, usize), CacheEntry>>>,
    cache_dir: std::path::PathBuf,
    processing_set: Arc<RwLock<HashSet<(String, usize)>>>,
    skipped_pages: Arc<RwLock<HashSet<(String, usize)>>>,
    failed_pages: Arc<RwLock<HashSet<(String, usize)>>>,
    completed_count: Arc<AtomicUsize>,
    skipped_count: Arc<AtomicUsize>,
    failed_count: Arc<AtomicUsize>,
    py_state: Arc<PyO3UpscalerState>,
    condition_settings: Arc<RwLock<ConditionalUpscaleSettings>>,
    conditions_list: Arc<RwLock<Vec<FrontendCondition>>>,
    default_timeout: f64,
    app: AppHandle,
) {
    let _ = worker_id; // é¿å…æœªä½¿ç”¨è­¦å‘Š

    while running.load(Ordering::SeqCst) {
        // å¦‚æœæœªå¯ç”¨è¶…åˆ†ï¼Œä¼‘çœ 
        if !enabled.load(Ordering::SeqCst) {
            thread::sleep(Duration::from_millis(100));
            continue;
        }

        // è·å–ä»»åŠ¡
        let task = get_highest_priority_task(&task_queue);

        if let Some(task) = task {
            // æ£€æŸ¥æ˜¯å¦åº”è¯¥å–æ¶ˆï¼ˆä¹¦ç±å·²åˆ‡æ¢ï¼‰
            let current = current_book
                .read()
                .ok()
                .and_then(|g| g.clone())
                .unwrap_or_default();
            if !task.book_path.is_empty() && task.book_path != current {
                log_debug!("â­ï¸ è·³è¿‡éå½“å‰ä¹¦ç±ä»»åŠ¡: {}", task.book_path);
                continue;
            }

            // æ ‡è®°ä¸ºæ­£åœ¨å¤„ç†
            if let Ok(mut set) = processing_set.write() {
                set.insert((task.book_path.clone(), task.page_index));
            }

            // å‘é€ processing çŠ¶æ€äº‹ä»¶åˆ°å‰ç«¯
            let processing_payload = UpscaleReadyPayload {
                book_path: task.book_path.clone(),
                page_index: task.page_index,
                image_hash: task.image_hash.clone(),
                status: UpscaleStatus::Processing,
                cache_path: None,
                error: None,
                original_size: None,
                upscaled_size: None,
                is_preload: task.score.priority != TaskPriority::Current,
                model_name: None,
                scale: None,
            };
            let _ = app.emit("upscale-ready", processing_payload);
            log_debug!("ğŸ“¤ å‘é€å¤„ç†ä¸­äº‹ä»¶: page {}", task.page_index);

            // å¤„ç†ä»»åŠ¡
            let result = process_task_v2(
                &py_state,
                &condition_settings,
                &conditions_list,
                &cache_dir,
                &cache_map,
                &task,
                default_timeout,
            );

            // ç§»é™¤å¤„ç†ä¸­æ ‡è®°
            if let Ok(mut set) = processing_set.write() {
                set.remove(&(task.book_path.clone(), task.page_index));
            }

            // æ‰“å°å¤„ç†ç»“æœ
            match &result {
                Ok(payload) => {
                    log_debug!(
                        "âœ… ä»»åŠ¡å¤„ç†å®Œæˆ: page {} status={:?}",
                        task.page_index,
                        payload.status
                    );
                }
                Err(e) => {
                    log_debug!("âŒ ä»»åŠ¡å¤„ç†å¤±è´¥: page {} error={}", task.page_index, e);
                }
            }

            // å¤„ç†ç»“æœå¹¶å‘é€äº‹ä»¶
            handle_task_result(
                result,
                &task,
                &completed_count,
                &skipped_count,
                &failed_count,
                &skipped_pages,
                &failed_pages,
                &app,
            );
        } else {
            // é˜Ÿåˆ—ä¸ºç©ºï¼ŒçŸ­æš‚ä¼‘çœ 
            thread::sleep(Duration::from_millis(20));
        }
    }
}

/// å¤„ç†ä»»åŠ¡ç»“æœå¹¶å‘é€äº‹ä»¶
fn handle_task_result(
    result: Result<UpscaleReadyPayload, String>,
    task: &UpscaleTask,
    completed_count: &Arc<AtomicUsize>,
    skipped_count: &Arc<AtomicUsize>,
    failed_count: &Arc<AtomicUsize>,
    skipped_pages: &Arc<RwLock<HashSet<(String, usize)>>>,
    failed_pages: &Arc<RwLock<HashSet<(String, usize)>>>,
    app: &AppHandle,
) {
    match result {
        Ok(payload) => {
            match payload.status {
                UpscaleStatus::Completed => {
                    completed_count.fetch_add(1, Ordering::SeqCst);
                    log_debug!("ğŸ“¤ å‘é€å®Œæˆäº‹ä»¶: page {}", task.page_index);
                }
                UpscaleStatus::Skipped => {
                    skipped_count.fetch_add(1, Ordering::SeqCst);
                    if let Ok(mut set) = skipped_pages.write() {
                        set.insert((task.book_path.clone(), task.page_index));
                    }
                    log_debug!(
                        "ğŸ“¤ å‘é€è·³è¿‡äº‹ä»¶: page {} reason={:?}",
                        task.page_index,
                        payload.error
                    );
                }
                UpscaleStatus::Failed => {
                    failed_count.fetch_add(1, Ordering::SeqCst);
                    if let Ok(mut set) = failed_pages.write() {
                        set.insert((task.book_path.clone(), task.page_index));
                    }
                    log_debug!(
                        "ğŸ“¤ å‘é€å¤±è´¥äº‹ä»¶: page {} error={:?}",
                        task.page_index,
                        payload.error
                    );
                }
                _ => {}
            }
            let _ = app.emit("upscale-ready", payload);
        }
        Err(e) => {
            failed_count.fetch_add(1, Ordering::SeqCst);
            if let Ok(mut set) = failed_pages.write() {
                set.insert((task.book_path.clone(), task.page_index));
            }

            log_debug!("ğŸ“¤ å‘é€é”™è¯¯äº‹ä»¶: page {} error={}", task.page_index, e);
            let payload = UpscaleReadyPayload {
                book_path: task.book_path.clone(),
                page_index: task.page_index,
                image_hash: task.image_hash.clone(),
                status: UpscaleStatus::Failed,
                cache_path: None,
                error: Some(e),
                original_size: None,
                upscaled_size: None,
                is_preload: task.score.priority != TaskPriority::Current,
                model_name: None,
                scale: None,
            };
            let _ = app.emit("upscale-ready", payload);
        }
    }
}
